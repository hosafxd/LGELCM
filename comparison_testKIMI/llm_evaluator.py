"""
LLM-Based Schema Evaluator
===========================
Basit ve anlaÅŸÄ±lÄ±r API kullanÄ±mÄ± ile medikal ÅŸema deÄŸerlendirme

Desteklenen modeller:
- Gemini (Google AI Studio)
- Gemma (Google AI Studio) 
- GLM (ZhipuAI)
- DeepSeek
- ve daha fazlasÄ±...

KULLANIM:
    # API keylerini tanÄ±mla
    API_KEYS = {
        "gemini": "AIzaSy...",
        "gemma": "KGAT_...",
        "glm": "sk-...",
        "deepseek": "sk-..."
    }
    
    # Evaluator oluÅŸtur
    evaluator = LLMEvaluator(
        model_type="gemini",
        model_name="gemini-1.5-flash",
        api_key=API_KEYS["gemini"]
    )
    
    # ÅžemalarÄ± deÄŸerlendir
    result = evaluator.evaluate_schema_pair(ground_truth, prediction, input_text)
"""

import json
import os
import time
from typing import Dict, List, Any, Optional
import logging
from google.genai import types
# Google AI iÃ§in
try:
    from google import genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš  google-genai kurulu deÄŸil. LÃ¼tfen Ã§alÄ±ÅŸtÄ±r:")
    print("  pip install google-genai")

# OpenAI-compatible API'ler iÃ§in
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš  openai kurulu deÄŸil. GLM/DeepSeek kullanmak iÃ§in:")
    print("  pip install openai")

logger = logging.getLogger(__name__)


class LLMEvaluator:
    """
    Basit ve anlaÅŸÄ±lÄ±r LLM-based ÅŸema deÄŸerlendirici
    
    Bu class direkt olarak API'leri kullanÄ±r, karmaÅŸÄ±k config yok!
    """
    
    # Her model iÃ§in rate limit ayarlarÄ± (saniye cinsinden bekleme sÃ¼resi)
    RATE_LIMITS = {
        "gemini-1.5-flash": 4.0,
        "gemini-2.5-flash": 5.0,
        "gemini-1.5-pro": 8.0,      
        "gemini-2.5-pro": 20.0,      
        "gemma-3-27b-it": 8.0,
        "glm-4-flash": 4.0,
        "deepseek-chat": 4.0
    }
    
    def __init__(self, 
                 model_type: str,
                 model_name: str,
                 api_key: str):
        self.model_type = model_type.lower()
        self.model_name = model_name
        self.api_key = api_key
        
        # FIX: Clean model name for rate limit lookup
        clean_name = model_name.replace("models/", "").replace("models/", "")
        
        # Debug print to verify
        print(f"   [DEBUG] Looking up rate limit for: '{clean_name}'")
        self.sleep_time = self.RATE_LIMITS.get(clean_name, 5.0)
        print(f"   [DEBUG] Found sleep time: {self.sleep_time}s")
        
        self._initialize_model()
        
        print(f"âœ“ {model_type} modeli baÅŸlatÄ±ldÄ±: {model_name}")
        print(f"  Rate limit: Her istek arasÄ± {self.sleep_time:.1f} saniye bekleme")
    
    def _initialize_model(self):
        if self.model_type in ["gemini", "gemma"]:
            if not GEMINI_AVAILABLE:
                raise ImportError("google-genai kurulu deÄŸil!")
            
            self.client = genai.Client(api_key=self.api_key)
            self.model = self.client
            
            # Model adÄ±nÄ± dÃ¼zelt (gemma iÃ§in hÃ¢lÃ¢ prefix gerekebiliyor)
            if self.model_type == "gemma":
                self.model_name = f"models/{self.model_name}"   # â† burayÄ± self.model_name olarak gÃ¼ncelle
            
          
                
        elif self.model_type in ["glm", "deepseek"]:
            if not OPENAI_AVAILABLE:
                raise ImportError("openai kurulu deÄŸil!")
            
            # API base URL'leri
            base_urls = {
                "glm": "https://open.bigmodel.cn/api/paas/v4/",
                "deepseek": "https://api.deepseek.com"
            }
            
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=base_urls.get(self.model_type)
            )
        
        else:
            raise ValueError(f"Desteklenmeyen model tipi: {self.model_type}")
            print("Desteklenen modeller: gemini, gemma, glm, deepseek")
    
    def evaluate_schema_pair(self, 
                           ground_truth: Dict,
                           prediction: Dict,
                           input_text: str) -> Dict[str, Any]:
        """
        Ä°ki ÅŸemayÄ± karÅŸÄ±laÅŸtÄ±r ve deÄŸerlendir
        
        Args:
            ground_truth: Referans ÅŸema (schema_train.json'dan)
            prediction: Modelinizin Ã¼rettiÄŸi ÅŸema
            input_text: Orijinal radyoloji metni
            
        Returns:
            DeÄŸerlendirme sonuÃ§larÄ±:
            {
                "similarity_score": 0.0-1.0,
                "clinical_equivalence": "exact|high|partial|low",
                "are_same_meaning": True/False,
                "entity_level_analysis": [...],
                "critical_errors": [...],
                "minor_differences": [...],
                "overall_assessment": "..."
            }
        """
        # DeÄŸerlendirme promptunu hazÄ±rla
        prompt = self._build_evaluation_prompt(ground_truth, prediction, input_text)
        
        # LLM'e sor
        response = self._generate(prompt)
        
        # CevabÄ± parse et
        result = self._parse_json_response(response)
        
        # Rate limit iÃ§in bekle
        time.sleep(self.sleep_time)
        
        return result
    
    def batch_evaluate(self, 
                      comparisons: List[Dict],
                      save_every: int = 10) -> List[Dict]:
        """
        Birden fazla ÅŸema Ã§iftini deÄŸerlendir
        
        Args:
            comparisons: [{"ground_truth": {...}, "prediction": {...}, "input": "..."}, ...]
            save_every: Her N deÄŸerlendirmede ara sonucu kaydet
            
        Returns:
            TÃ¼m deÄŸerlendirme sonuÃ§larÄ± listesi
        """
        results = []
        
        print(f"\nðŸ”„ {len(comparisons)} ÅŸema Ã§ifti deÄŸerlendiriliyor...")
        print(f"â± Tahmini sÃ¼re: {len(comparisons) * self.sleep_time / 60:.1f} dakika")
        
        for idx, comp in enumerate(comparisons):
            try:
                result = self.evaluate_schema_pair(
                    comp['ground_truth'],
                    comp['prediction'],
                    comp['input']
                )
                results.append(result)
                
                # Ä°lerleme gÃ¶ster
                if (idx + 1) % 5 == 0:
                    print(f"  âœ“ {idx + 1}/{len(comparisons)} tamamlandÄ±")
                
                # Ara sonuÃ§larÄ± kaydet
                if (idx + 1) % save_every == 0:
                    self._save_intermediate(results, idx + 1)
                
            except Exception as e:
                print(f"  âš  Hata (sample {idx}): {e}")
                results.append({
                    'error': str(e),
                    'sample_idx': idx
                })
        
        print(f"âœ“ DeÄŸerlendirme tamamlandÄ±!")
        return results
    def _safe_extract_gemini_text(self, response) -> Optional[str]:
        """
        Gemini response'tan gÃ¼venli ÅŸekilde text Ã§Ä±kar
        (finish_reason=2 gibi durumlarÄ± tolere eder)
        """
        try:
            if not hasattr(response, "candidates") or not response.candidates:
                return None

            candidate = response.candidates[0]

            if not hasattr(candidate, "content") or not candidate.content:
                return None

            parts = candidate.content.parts
            if not parts:
                return None

            texts = []
            for part in parts:
                if hasattr(part, "text") and part.text:
                    texts.append(part.text)

            return "\n".join(texts) if texts else None

        except Exception:
            return None


    def _generate(self, prompt: str) -> str:
        """
        SeÃ§ilen model ile text Ã¼ret
        
        Her model tipi iÃ§in farklÄ± API Ã§aÄŸrÄ±sÄ±
        """
        try:
            if self.model_type in ["gemini", "gemma"]:
                return self._generate_gemini(prompt)
            
            elif self.model_type in ["glm", "deepseek"]:
                return self._generate_openai_compatible(prompt)
            
            else:
                raise ValueError(f"Model tipi desteklenmiyor: {self.model_type}")
        
        except Exception as e:
            logger.error(f"LLM generation hatasÄ±: {e}")
            raise
    
    def _generate_gemini(self, prompt: str) -> str:

        import time
        
        max_retries = 3
        base_wait = self.sleep_time
        
        for attempt in range(max_retries):
            try:
                # FIX: safety_settings goes INSIDE config, not as separate parameter
                generation_config = types.GenerateContentConfig(
                    temperature=0.1,
                    top_p=0.85,
                    max_output_tokens=4096,
                    safety_settings=[
                        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                    ]
                )
                
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=generation_config,  # Safety settings are inside here now
                )
                
                text = self._safe_extract_gemini_text(response)
                if text:
                    time.sleep(self.sleep_time)
                    return text
                
                if attempt < max_retries - 1:
                    wait_time = base_wait * (2 ** attempt)
                    print(f"  âš ï¸ Empty response, retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    raise Exception("Empty response after all retries")
            
            except Exception as e:
                error_str = str(e)
                print(f"  [DEBUG] Error: {error_str[:80]}")
                
                if "429" in error_str:
                    wait_time = base_wait * (3 ** attempt)
                    print(f"  âš ï¸ Rate limit (429), backing off {wait_time}s...")
                    time.sleep(wait_time)
                elif attempt < max_retries - 1:
                    wait_time = base_wait * (2 ** attempt)
                    print(f"  âš ï¸ Retrying in {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    raise Exception(f"Failed after {max_retries} attempts: {e}")
        
        raise Exception("Unexpected error")

    
    def _generate_openai_compatible(self, prompt: str) -> str:
        """GLM, DeepSeek gibi OpenAI-compatible API'ler iÃ§in Ã¼retim"""
        
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=2048
        )
        
        return response.choices[0].message.content.strip()
    
    def _build_evaluation_prompt(self, 
                                 gt: Dict, 
                                 pred: Dict, 
                                 input_text: str) -> str:
        """
        DeÄŸerlendirme promptunu oluÅŸtur
        
        LLM'e hangi kriterlere gÃ¶re deÄŸerlendireceÄŸini sÃ¶ylÃ¼yoruz
        """
        
        # Ground truth ve prediction'Ä± JSON string'e Ã§evir
        gt_output = json.dumps(gt.get('output', []), indent=2)
        pred_output = json.dumps(pred.get('output', []), indent=2)
        
        prompt = f"""Compare two medical entity extractions and return ONLY valid JSON.

INPUT TEXT:
{input_text[:500]}

REFERENCE:
{gt_output}

PREDICTION:
{pred_output}

Return this JSON structure (no extra text, no markdown):
{{
  "similarity_score": 0.0-1.0,
  "clinical_equivalence": "exact|high|partial|low",
  "are_same_meaning": true or false,
  "key_differences": ["brief list"],
  "overall_assessment": "one sentence"
}}

Rules:
- Return ONLY the JSON object
- No backticks, no markdown
- Keep assessment brief
- Focus on clinical meaning, not exact wording
"""
    
        return prompt
    
    # llm_evaluator.py â†’ _parse_json_response() dÃ¼zelt

    def _parse_json_response(self, response: str) -> Dict[str, Any]:
        """LLM cevabÄ±nÄ± parse et - robust version"""
        
        # Temizle
        response = response.strip()
        
        # Markdown kaldÄ±r
        if response.startswith('```json'):
            response = response.replace('```json', '', 1)
        if response.startswith('```'):
            response = response.replace('```', '', 1)
        if response.endswith('```'):
            response = response.rsplit('```', 1)[0]
        
        response = response.strip()
        
        # â­ FIX 1: Incomplete JSON repair
        # EÄŸer son } yoksa, ekle
        if response.count('{') > response.count('}'):
            response += '}' * (response.count('{') - response.count('}'))
        
        # â­ FIX 2: Unterminated strings
        # Son satÄ±rda tÄ±rnak aÃ§Ä±ksa kapat
        lines = response.split('\n')
        fixed_lines = []
        for line in lines:
            # Tek tÄ±rnak sayÄ±sÄ±
            quote_count = line.count('"') - line.count('\\"')
            if quote_count % 2 == 1:  # Tek sayÄ±da tÄ±rnak
                # SatÄ±r sonu virgÃ¼l varsa kaldÄ±r, yoksa ekle
                if line.rstrip().endswith(','):
                    line = line.rstrip()[:-1] + '",'
                else:
                    line = line.rstrip() + '"'
            fixed_lines.append(line)
        
        response = '\n'.join(fixed_lines)
        
        # JSON parse
        try:
            parsed = json.loads(response)
            return parsed
        
        except json.JSONDecodeError as e:
            print(f"âš  JSON parse hatasÄ±: {e}")
            print(f"Raw response (ilk 500 char): {response[:500]}")
            
            # â­ FIX 3: Fallback - minimal valid response
            try:
                # Extract similarity_score en azÄ±ndan
                import re
                sim_match = re.search(r'"similarity_score":\s*([0-9.]+)', response)
                sim_score = float(sim_match.group(1)) if sim_match else 0.0
                
                equiv_match = re.search(r'"clinical_equivalence":\s*"(\w+)"', response)
                equiv = equiv_match.group(1) if equiv_match else 'unknown'
                
                return {
                    'similarity_score': sim_score,
                    'clinical_equivalence': equiv,
                    'are_same_meaning': False,
                    'entity_level_analysis': [],
                    'critical_errors': ['Incomplete LLM response - partial data'],
                    'minor_differences': [],
                    'overall_assessment': 'Evaluation incomplete due to parse error',
                    'raw_response': response[:500],
                    'parse_error': str(e)
                }
            except:
                # Son Ã§are - tamamen boÅŸ
                return {
                    'similarity_score': 0.0,
                    'clinical_equivalence': 'unknown',
                    'are_same_meaning': False,
                    'entity_level_analysis': [],
                    'critical_errors': ['Complete JSON parse failure'],
                    'minor_differences': [],
                    'overall_assessment': 'Evaluation failed',
                    'raw_response': response[:500],
                    'parse_error': str(e)
                }
    
    def _save_intermediate(self, results: List[Dict], count: int):
        """Ara sonuÃ§larÄ± kaydet (hata durumunda kaybetmemek iÃ§in)"""
        filename = f'llm_eval_intermediate_{count}.json'
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"  ðŸ’¾ Ara sonuÃ§ kaydedildi: {filename}")


# ============================================================================
# Semantic Similarity (Embedding-based)
# ============================================================================

class EmbeddingBasedEvaluator:
    """
    Sentence embeddings kullanarak semantik benzerlik hesapla
    
    Bu method API Ã§aÄŸrÄ±sÄ± gerektirmez, tamamen local Ã§alÄ±ÅŸÄ±r!
    """
    
    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):
        """
        Args:
            model_name: HuggingFace model adÄ±
                       - 'sentence-transformers/all-MiniLM-L6-v2' (genel, hÄ±zlÄ±)
                       - 'dmis-lab/biobert-base-cased-v1.2' (medikal, daha iyi)
        """
        try:
            from sentence_transformers import SentenceTransformer
            self.model = SentenceTransformer(model_name)
            print(f"âœ“ Embedding modeli yÃ¼klendi: {model_name}")
        except ImportError:
            print("âš  sentence-transformers kurulu deÄŸil!")
            print("  pip install sentence-transformers")
            self.model = None
    
    def compute_schema_similarity(self, schema1: Dict, schema2: Dict) -> float:
        """
        Ä°ki ÅŸema arasÄ±nda semantik benzerlik hesapla
        
        Returns:
            0.0 ile 1.0 arasÄ± similarity score
        """
        if not self.model:
            return 0.0
        
        # ÅžemalarÄ± text'e Ã§evir
        text1 = self._schema_to_text(schema1)
        text2 = self._schema_to_text(schema2)
        
        # Embeddings hesapla
        embeddings = self.model.encode([text1, text2])
        
        # Cosine similarity
        from sklearn.metrics.pairwise import cosine_similarity
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        
        return float(similarity)
    
    def _schema_to_text(self, schema: Dict) -> str:
        """
        ÅžemayÄ± text'e Ã§evir (embedding iÃ§in)
        
        TÃ¼m field'larÄ± anlamlÄ± bir text'e dÃ¶nÃ¼ÅŸtÃ¼r
        """
        output = schema.get('output', [])
        
        texts = []
        for entity in output:
            parts = []
            
            # Her field'Ä± ekle
            if entity.get('abnormality') and entity['abnormality'] != 'None':
                parts.append(f"abnormality: {entity['abnormality']}")
            
            if entity.get('presence') and entity['presence'] != 'None':
                parts.append(f"presence: {entity['presence']}")
            
            if entity.get('location'):
                loc = entity['location']
                if isinstance(loc, list):
                    parts.append(f"location: {', '.join(loc)}")
                else:
                    parts.append(f"location: {loc}")
            
            if entity.get('degree') and entity['degree'] != 'None':
                deg = entity['degree']
                if isinstance(deg, list):
                    parts.append(f"degree: {', '.join(deg)}")
                else:
                    parts.append(f"degree: {deg}")
            
            if entity.get('measurement') and entity['measurement'] != 'None':
                parts.append(f"measurement: {entity['measurement']}")
            
            if parts:
                texts.append(' | '.join(parts))
        
        return ' ; '.join(texts)


# ============================================================================
# KullanÄ±m Ã–rnekleri
# ============================================================================

def example_basic_usage():
    """
    Ã–RNEK 1: Temel kullanÄ±m (sizin kod yapÄ±nÄ±za uygun)
    """
    print("\n" + "="*70)
    print("Ã–RNEK 1: Temel LLM DeÄŸerlendirme")
    print("="*70)
    
    # API keylerini tanÄ±mla
    API_KEYS = {
        "gemini": "AIzaSyDKfk3iyWUilm8SU-f70PSRjo9etZBxrDk",
        "gemma": "KGAT_7b8482384bb20717b1fa8b9c914ff365",
        "glm": "sk-t80kLqA1bkLIoTi0x0vjmno3-gbMvrX3A44SOh4QWHRpiYJvMeOTpUOScAAWzOPzpDxC8AyC0KPdgaqHrn_5RPa_RhY_",
        "deepseek": "sk-450186e490b34beb8347badc0fa91e6b",
    }


    
    # Evaluator oluÅŸtur (model seÃ§imi Ã§ok basit!)
    evaluator = LLMEvaluator(
        model_type="gemini",              # "gemini", "gemma", "glm", "deepseek"
        model_name="models/gemini-2.5-flash",    # Model adÄ±
        api_key=API_KEYS["gemini"]        # API key
    )
    
    # Ã–rnek ÅŸemalar
    ground_truth = {
        'input': 'Spleen size is enlarged and measured 134 mm.',
        'output': [{
            'abnormality': 'enlarged',
            'presence': 'present',
            'location': ['spleen'],
            'degree': 'None',
            'measurement': '134 mm',
            'comparison': 'None'
        }]
    }
    
    prediction = {
        'input': 'Spleen size is enlarged and measured 134 mm.',
        'output': [{
            'abnormality': 'splenomegaly',  # FarklÄ± kelime, aynÄ± anlam
            'presence': 'present',
            'location': ['spleen'],
            'degree': 'None',
            'measurement': '134mm',  # BoÅŸluk farkÄ±
            'comparison': 'None'
        }]
    }
    
    # DeÄŸerlendir!
    result = evaluator.evaluate_schema_pair(
        ground_truth,
        prediction,
        ground_truth['input']
    )
    
    # SonuÃ§larÄ± yazdÄ±r
    print(f"\nðŸ“Š SONUÃ‡LAR:")
    print(f"  Benzerlik skoru: {result.get('similarity_score', 0):.3f}")
    print(f"  Klinik eÅŸdeÄŸerlik: {result.get('clinical_equivalence', 'unknown')}")
    print(f"  AynÄ± anlama mÄ± geliyor: {result.get('are_same_meaning', False)}")
    print(f"\n  DeÄŸerlendirme: {result.get('overall_assessment', 'N/A')}")


def example_batch_evaluation():
    """
    Ã–RNEK 2: Toplu deÄŸerlendirme (sizin map_schema yapÄ±nÄ±z gibi)
    """
    print("\n" + "="*70)
    print("Ã–RNEK 2: Toplu DeÄŸerlendirme")
    print("="*70)
    
    # API key
    API_KEY = ""
    
    # Evaluator
    evaluator = LLMEvaluator(
        model_type="gemini",
        model_name="models/gemini-2.5-flash",
        api_key=API_KEY
    )
    
    # Ground truth ve predictions yÃ¼kle
    with open('./data/0_normalized/gt0.json', 'r') as f:
        ground_truths = json.load(f)
    
    with open('./data/0_normalized/sample0.0.json', 'r') as f:
        predictions = json.load(f)
    
    # KarÅŸÄ±laÅŸtÄ±rma listesi hazÄ±rla
    comparisons = []
    for gt in ground_truths:
        # Matching prediction bul (input'a gÃ¶re)
        pred = next(
            (p for p in predictions if p['input'] == gt['input']),
            None
        )
        
        if pred:
            comparisons.append({
                'ground_truth': gt,
                'prediction': pred,
                'input': gt['input']
            })
    
    # Toplu deÄŸerlendirme (sizin map_schema gibi)
    results = evaluator.batch_evaluate(comparisons)
    
    # SonuÃ§larÄ± kaydet
    with open('llm_evaluation_results.json', 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ {len(results)} deÄŸerlendirme tamamlandÄ±!")


def example_multi_model_comparison():
    """
    Ã–RNEK 3: Birden fazla model ile deÄŸerlendirme
    """
    print("\n" + "="*70)
    print("Ã–RNEK 3: Multi-Model KarÅŸÄ±laÅŸtÄ±rma")
    print("="*70)
    
    # TÃ¼m API keyler
    API_KEYS = {
        "gemini": "AIzaSyDKfk3iyWUilm8SU-f70PSRjo9etZBxrDk",
        "gemma": "KGAT_7b8482384bb20717b1fa8b9c914ff365",
    }
    
    # Ã–rnek ÅŸema Ã§ifti
    gt = {...}
    pred = {...}
    
    # Her model ile deÄŸerlendir
    results = {}
    
    for model_type, api_key in API_KEYS.items():
        print(f"\nðŸ”„ {model_type} ile deÄŸerlendiriliyor...")
        
        evaluator = LLMEvaluator(
            model_type=model_type,
            model_name=f"{model_type}-1.5-flash",
            api_key=api_key
        )
        
        result = evaluator.evaluate_schema_pair(gt, pred, gt['input'])
        results[model_type] = result
    
    # KarÅŸÄ±laÅŸtÄ±r
    print(f"\nðŸ“Š MODEL KARÅžILAÅžTIRMASI:")
    for model, result in results.items():
        print(f"  {model:10s}: similarity={result['similarity_score']:.3f}")


if __name__ == '__main__':
    
    # Ã–rnekleri Ã§alÄ±ÅŸtÄ±rmak isterseniz uncomment edin:
    # example_basic_usage()
    # example_batch_evaluation()
    # example_multi_model_comparison()
    pass