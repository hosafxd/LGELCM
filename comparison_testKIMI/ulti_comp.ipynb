{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217dbb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing report-level schemas...\n",
      "\n",
      "======================================================================\n",
      "REPORT-LEVEL SCHEMA NORMALIZATION\n",
      "======================================================================\n",
      "\n",
      "Base directory: data_report\n",
      "Found 4 data directories: ['0', '1', '2', '3']\n",
      "\n",
      "======================================================================\n",
      "Processing: data_report/0\n",
      "======================================================================\n",
      "Found 2 JSON files\n",
      "  ‚úÖ gt0.json (NEW format normalized)\n",
      "  ‚úÖ sample0.2.json (NEW format normalized)\n",
      "\n",
      "======================================================================\n",
      "Processing: data_report/1\n",
      "======================================================================\n",
      "Found 1 JSON files\n",
      "  ‚úÖ gt1.json (NEW format normalized)\n",
      "\n",
      "======================================================================\n",
      "Processing: data_report/2\n",
      "======================================================================\n",
      "Found 1 JSON files\n",
      "  ‚úÖ gt2.json (NEW format normalized)\n",
      "\n",
      "======================================================================\n",
      "Processing: data_report/3\n",
      "======================================================================\n",
      "Found 1 JSON files\n",
      "  ‚úÖ gt3.json (NEW format normalized)\n",
      "\n",
      "======================================================================\n",
      "NORMALIZATION SUMMARY\n",
      "======================================================================\n",
      "Processed: 5\n",
      "Errors: 0\n",
      "\n",
      "‚úÖ ALL SCHEMAS NORMALIZED!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 0: ENHANCED JSON NORMALIZATION FOR REPORT-LEVEL SCHEMAS\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Normalizes both OLD and NEW schema formats:\n",
    "- OLD: {\"input\": \"...\", \"output\": [...]}\n",
    "- NEW: {\"report\": \"...\", \"inputs\": [{\"input\": \"...\", \"output\": [...]}]}\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "# ============================================================================\n",
    "# CANONICAL ENTITY FOR NEW FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "CANONICAL_ENTITY_NEW = {\n",
    "    \"general_finding\": None,\n",
    "    \"specific_finding\": None,\n",
    "    \"finding_presence\": \"unknown\",  # present/absent/uncertain\n",
    "    \"location\": [],\n",
    "    \"degree\": [],\n",
    "    \"measurement\": None,\n",
    "    \"comparison\": None\n",
    "}\n",
    "\n",
    "def normalize_entity_new_format(entity: dict) -> dict:\n",
    "    \"\"\"Normalize entity for NEW schema format\"\"\"\n",
    "    normalized = deepcopy(CANONICAL_ENTITY_NEW)\n",
    "    \n",
    "    for key in normalized:\n",
    "        if key not in entity:\n",
    "            continue\n",
    "        \n",
    "        value = entity[key]\n",
    "        \n",
    "        # \"None\" string ‚Üí None\n",
    "        if isinstance(value, str) and value.lower() == \"none\":\n",
    "            value = None\n",
    "        \n",
    "        # Empty string ‚Üí None\n",
    "        if isinstance(value, str) and value.strip() == \"\":\n",
    "            value = None\n",
    "        \n",
    "        # location & degree MUST be list\n",
    "        if key in (\"location\", \"degree\"):\n",
    "            if value is None:\n",
    "                value = []\n",
    "            elif isinstance(value, str):\n",
    "                value = [value] if value.strip() else []\n",
    "            elif not isinstance(value, list):\n",
    "                value = [str(value)]\n",
    "            # Remove \"None\" from lists\n",
    "            value = [v for v in value if str(v).lower() != \"none\"]\n",
    "        \n",
    "        # finding_presence normalize\n",
    "        if key == \"finding_presence\":\n",
    "            if value is None:\n",
    "                value = \"unknown\"\n",
    "            elif isinstance(value, str):\n",
    "                value = value.lower()\n",
    "                if value not in (\"present\", \"absent\", \"uncertain\"):\n",
    "                    value = \"unknown\"\n",
    "        \n",
    "        normalized[key] = value\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def detect_schema_format(data: dict) -> str:\n",
    "    \"\"\"Detect if schema is OLD or NEW format\"\"\"\n",
    "    if \"report\" in data and \"inputs\" in data:\n",
    "        return \"new\"\n",
    "    elif \"input\" in data and \"output\" in data:\n",
    "        return \"old\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "def normalize_new_format_schema(data: dict) -> dict:\n",
    "    \"\"\"Normalize NEW format schema\"\"\"\n",
    "    normalized_inputs = []\n",
    "    \n",
    "    for input_item in data.get(\"inputs\", []):\n",
    "        normalized_entities = []\n",
    "        \n",
    "        for entity in input_item.get(\"output\", []):\n",
    "            normalized_entities.append(normalize_entity_new_format(entity))\n",
    "        \n",
    "        normalized_inputs.append({\n",
    "            \"input\": input_item.get(\"input\", \"\").strip(),\n",
    "            \"output\": normalized_entities\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"report\": data.get(\"report\", \"\").strip(),\n",
    "        \"inputs\": normalized_inputs\n",
    "    }\n",
    "\n",
    "def auto_normalize_report_schemas(base_dir: str = \"data_report\"):\n",
    "    \"\"\"\n",
    "    Auto-normalize all report-level schemas\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"‚ùå Base directory not found: {base_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Find all numbered directories\n",
    "    data_dirs = sorted([d for d in base_path.iterdir() if d.is_dir() and d.name.isdigit()])\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"REPORT-LEVEL SCHEMA NORMALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nBase directory: {base_dir}\")\n",
    "    print(f\"Found {len(data_dirs)} data directories: {[d.name for d in data_dirs]}\")\n",
    "    \n",
    "    total_processed = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    for data_dir in data_dirs:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Processing: {data_dir}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Find all JSON files\n",
    "        json_files = list(data_dir.glob(\"gt*.json\")) + list(data_dir.glob(\"sample*.json\"))\n",
    "        \n",
    "        if not json_files:\n",
    "            print(f\"‚ö† No JSON files found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found {len(json_files)} JSON files\")\n",
    "        \n",
    "        for json_file in sorted(json_files):\n",
    "            try:\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    raw_data = json.load(f)\n",
    "                \n",
    "                # Detect format\n",
    "                schema_format = detect_schema_format(raw_data)\n",
    "                \n",
    "                if schema_format == \"new\":\n",
    "                    # Normalize\n",
    "                    normalized = normalize_new_format_schema(raw_data)\n",
    "                    \n",
    "                    # Overwrite original (or save to new location)\n",
    "                    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(normalized, f, indent=2, ensure_ascii=False)\n",
    "                    \n",
    "                    print(f\"  ‚úÖ {json_file.name} (NEW format normalized)\")\n",
    "                    total_processed += 1\n",
    "                    \n",
    "                elif schema_format == \"old\":\n",
    "                    print(f\"  ‚ÑπÔ∏è  {json_file.name} (OLD format - skipped)\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è  {json_file.name} (Unknown format)\")\n",
    "                    total_errors += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå {json_file.name}: {e}\")\n",
    "                total_errors += 1\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"NORMALIZATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Processed: {total_processed}\")\n",
    "    print(f\"Errors: {total_errors}\")\n",
    "    \n",
    "    return total_processed, total_errors\n",
    "\n",
    "# RUN\n",
    "print(\"üîÑ Normalizing report-level schemas...\\n\")\n",
    "processed, errors = auto_normalize_report_schemas(\"data_report\")\n",
    "\n",
    "if errors == 0:\n",
    "    print(\"\\n‚úÖ ALL SCHEMAS NORMALIZED!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  {errors} errors occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c38e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosafxd/Downloads/D√ñNEM6/MEDICAL_IMAGING/RaTEScore/venv310/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ sentence-transformers y√ºklendi!\n",
      "‚úÖ Evaluator initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from medical_schema_evaluator import MedicalSchemaEvaluator\n",
    "import importlib\n",
    "import medical_schema_evaluator\n",
    "\n",
    "importlib.reload(medical_schema_evaluator)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"‚úÖ sentence-transformers y√ºklendi!\")\n",
    "\n",
    "# Test initialization\n",
    "evaluator = MedicalSchemaEvaluator()\n",
    "print(\"‚úÖ Evaluator initialized successfully!\")\n",
    "\n",
    "try:\n",
    "    from google import genai\n",
    "    from google.genai import types  # Move this inside here\n",
    "    GEMINI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEMINI_AVAILABLE = False\n",
    "    types = None  # Define as None if import fails\n",
    "    print(\"‚ö† google-genai kurulu deƒüil. L√ºtfen √ßalƒ±≈ütƒ±r:\")\n",
    "    print(\"  pip install google-genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "059f7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosafxd/Downloads/D√ñNEM6/MEDICAL_IMAGING/RaTEScore/venv310/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ENTITY-LEVEL EVALUATION v2.0 (Semantic Matching)\n",
      "======================================================================\n",
      "\n",
      "üìÇ Data Directory: ./data_report/0/\n",
      "üìÑ Ground Truth: gt0.json\n",
      "\n",
      "ü§ñ Selected LLM Models (1):\n",
      "   ‚úÖ gemini_pro: models/gemini-2.5-pro\n",
      "\n",
      "üß† Selected Embedding Models (4):\n",
      "   ‚úÖ pubmedbert: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "   ‚úÖ s_pubmedbert: pritamdeka/S-PubMedBert-MS-MARCO\n",
      "   ‚úÖ general_baseline: sentence-transformers/all-MiniLM-L6-v2\n",
      "   ‚úÖ neuml_pubmedbert: NeuML/pubmedbert-base-embeddings\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üöÄ STARTING EVALUATION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "BATCH EVALUATION\n",
      "======================================================================\n",
      "Directory: ./data_report/0/\n",
      "GT: gt0.json\n",
      "Test files: 4\n",
      "\n",
      "üîß Initializing Semantic Medical Matcher...\n",
      "üîÑ Loading model: cambridgeltl/SapBERT-from-PubMedBERT-fulltext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name cambridgeltl/SapBERT-from-PubMedBERT-fulltext. Creating a new one with mean pooling.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 706.32it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: cambridgeltl/SapBERT-from-PubMedBERT-fulltext\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SapBERT loaded\n",
      "üîÑ Loading model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext. Creating a new one with mean pooling.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 1123.92it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.decoder.bias               | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PubMedBERT loaded\n",
      "‚ö° Using cached model: cambridgeltl/SapBERT-from-PubMedBERT-fulltext\n",
      "‚úÖ SapBERT loaded\n",
      "‚ö° Using cached model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "‚úÖ PubMedBERT loaded\n",
      "üß† Pre-loading embedding models...\n",
      "    Loading: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext. Creating a new one with mean pooling.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 1389.71it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.decoder.bias               | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "   ‚úÖ Cached: pubmedbert\n",
      "    Loading: pritamdeka/S-PubMedBert-MS-MARCO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 1634.87it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: pritamdeka/S-PubMedBert-MS-MARCO\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "   ‚úÖ Cached: s_pubmedbert\n",
      "    Loading: sentence-transformers/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1472.14it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "   ‚úÖ Cached: general_baseline\n",
      "    Loading: NeuML/pubmedbert-base-embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 1464.78it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "   ‚úÖ Cached: neuml_pubmedbert\n",
      "‚úÖ Evaluator ready!\n",
      "\n",
      "======================================================================\n",
      "üìÅ Evaluating: sample0.0.json\n",
      "======================================================================\n",
      "üìä Entity Count: GT=30 | Pred=30\n",
      "\n",
      "üìà STRUCTURAL (Entity-Level):\n",
      "   Precision: 0.900\n",
      "   Recall:    0.900\n",
      "   F1-Score:  0.900\n",
      "   TP: 27 | FP: 3 | FN: 3\n",
      "\n",
      "üîç SEMANTIC (Embedding) Evaluation:\n",
      "   Using cached pubmedbert...\n",
      "      ‚úÖ TP: 0.998 (n=27)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.995 (n=3)\n",
      "      ‚ö†Ô∏è  FN (sem): 1.000 (n=3)\n",
      "   Using cached s_pubmedbert...\n",
      "      ‚úÖ TP: 0.992 (n=27)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.981 (n=3)\n",
      "      ‚ö†Ô∏è  FN (sem): 1.000 (n=3)\n",
      "   Using cached general_baseline...\n",
      "      ‚úÖ TP: 0.954 (n=27)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.900 (n=3)\n",
      "      ‚ö†Ô∏è  FN (sem): 1.000 (n=3)\n",
      "   Using cached neuml_pubmedbert...\n",
      "      ‚úÖ TP: 0.956 (n=27)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.886 (n=3)\n",
      "      ‚ö†Ô∏è  FN (sem): 1.000 (n=3)\n",
      "\n",
      "ü§ñ LLM Evaluation:\n",
      "   Testing gemini_pro...    [DEBUG] Looking up rate limit for: 'gemini-2.5-pro'\n",
      "   [DEBUG] Found sleep time: 20.0s\n",
      "‚úì gemini modeli ba≈ülatƒ±ldƒ±: models/gemini-2.5-pro\n",
      "  Rate limit: Her istek arasƒ± 20.0 saniye bekleme\n",
      "  [DEBUG] Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'This model is currently exp\n",
      "  ‚ö†Ô∏è Retrying in 20.0s...\n",
      "‚úÖ 0.920 (n=5)\n",
      "\n",
      "üîç Sample Mismatches:\n",
      "   FN: Surface irregularity @ ['thoracic aorta']\n",
      "   FN: Surface irregularity @ ['thoracic aorta']\n",
      "   FP: Aortic dilatation @ ['ascending aorta']\n",
      "   FP: Surface irregularity @ ['abdominal aorta']\n",
      "\n",
      "  üíæ Saved: result_sample0.0.json\n",
      "\n",
      "======================================================================\n",
      "üìÅ Evaluating: sample0.1.json\n",
      "======================================================================\n",
      "üìä Entity Count: GT=30 | Pred=30\n",
      "\n",
      "üìà STRUCTURAL (Entity-Level):\n",
      "   Precision: 0.300\n",
      "   Recall:    0.300\n",
      "   F1-Score:  0.300\n",
      "   TP: 9 | FP: 21 | FN: 21\n",
      "\n",
      "üîç SEMANTIC (Embedding) Evaluation:\n",
      "   Using cached pubmedbert...\n",
      "      ‚úÖ TP: 0.982 (n=9)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.991 (n=21)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.990 (n=21)\n",
      "   Using cached s_pubmedbert...\n",
      "      ‚úÖ TP: 0.931 (n=9)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.960 (n=21)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.960 (n=21)\n",
      "   Using cached general_baseline...\n",
      "      ‚úÖ TP: 0.644 (n=9)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.762 (n=21)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.770 (n=21)\n",
      "   Using cached neuml_pubmedbert...\n",
      "      ‚úÖ TP: 0.642 (n=9)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.815 (n=21)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.813 (n=21)\n",
      "\n",
      "ü§ñ LLM Evaluation:\n",
      "   Testing gemini_pro...    [DEBUG] Looking up rate limit for: 'gemini-2.5-pro'\n",
      "   [DEBUG] Found sleep time: 20.0s\n",
      "‚úì gemini modeli ba≈ülatƒ±ldƒ±: models/gemini-2.5-pro\n",
      "  Rate limit: Her istek arasƒ± 20.0 saniye bekleme\n",
      "  [DEBUG] Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'This model is currently exp\n",
      "  ‚ö†Ô∏è Retrying in 20.0s...\n",
      "‚úÖ 0.040 (n=5)\n",
      "\n",
      "üîç Sample Mismatches:\n",
      "   FN: Effusion @ ['pericardium']\n",
      "   FN: Calcification @ ['coronary artery']\n",
      "   FP: Thyroid abnormality @ ['unspecified region']\n",
      "   FP: Effusion @ ['unspecified region']\n",
      "\n",
      "  üíæ Saved: result_sample0.1.json\n",
      "\n",
      "======================================================================\n",
      "üìÅ Evaluating: sample0.2.json\n",
      "======================================================================\n",
      "üìä Entity Count: GT=30 | Pred=30\n",
      "\n",
      "üìà STRUCTURAL (Entity-Level):\n",
      "   Precision: 0.733\n",
      "   Recall:    0.733\n",
      "   F1-Score:  0.733\n",
      "   TP: 22 | FP: 8 | FN: 8\n",
      "\n",
      "üîç SEMANTIC (Embedding) Evaluation:\n",
      "   Using cached pubmedbert...\n",
      "      ‚úÖ TP: 0.993 (n=22)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.995 (n=8)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.998 (n=8)\n",
      "   Using cached s_pubmedbert...\n",
      "      ‚úÖ TP: 0.972 (n=22)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.975 (n=8)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.990 (n=8)\n",
      "   Using cached general_baseline...\n",
      "      ‚úÖ TP: 0.871 (n=22)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.837 (n=8)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.937 (n=8)\n",
      "   Using cached neuml_pubmedbert...\n",
      "      ‚úÖ TP: 0.861 (n=22)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.888 (n=8)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.953 (n=8)\n",
      "\n",
      "ü§ñ LLM Evaluation:\n",
      "   Testing gemini_pro...    [DEBUG] Looking up rate limit for: 'gemini-2.5-pro'\n",
      "   [DEBUG] Found sleep time: 20.0s\n",
      "‚úì gemini modeli ba≈ülatƒ±ldƒ±: models/gemini-2.5-pro\n",
      "  Rate limit: Her istek arasƒ± 20.0 saniye bekleme\n",
      "‚úÖ 0.540 (n=5)\n",
      "\n",
      "üîç Sample Mismatches:\n",
      "   FN: Anatomical variant @ ['aortic arch']\n",
      "   FN: Airway obstruction @ ['trachea', 'central airways']\n",
      "   FP: Cardiomegaly @ ['heart']\n",
      "   FP: Lymphadenopathy @ ['mediastinal', 'axillary', 'hilar']\n",
      "\n",
      "  üíæ Saved: result_sample0.2.json\n",
      "\n",
      "======================================================================\n",
      "üìÅ Evaluating: sample0.3.json\n",
      "======================================================================\n",
      "üìä Entity Count: GT=30 | Pred=18\n",
      "\n",
      "üìà STRUCTURAL (Entity-Level):\n",
      "   Precision: 0.333\n",
      "   Recall:    0.200\n",
      "   F1-Score:  0.250\n",
      "   TP: 6 | FP: 12 | FN: 24\n",
      "\n",
      "üîç SEMANTIC (Embedding) Evaluation:\n",
      "   Using cached pubmedbert...\n",
      "      ‚úÖ TP: 0.986 (n=6)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.990 (n=12)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.989 (n=24)\n",
      "   Using cached s_pubmedbert...\n",
      "      ‚úÖ TP: 0.945 (n=6)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.956 (n=12)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.955 (n=24)\n",
      "   Using cached general_baseline...\n",
      "      ‚úÖ TP: 0.676 (n=6)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.722 (n=12)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.745 (n=24)\n",
      "   Using cached neuml_pubmedbert...\n",
      "      ‚úÖ TP: 0.701 (n=6)\n",
      "      ‚ö†Ô∏è  FP (sem): 0.806 (n=12)\n",
      "      ‚ö†Ô∏è  FN (sem): 0.769 (n=24)\n",
      "\n",
      "ü§ñ LLM Evaluation:\n",
      "   Testing gemini_pro...    [DEBUG] Looking up rate limit for: 'gemini-2.5-pro'\n",
      "   [DEBUG] Found sleep time: 20.0s\n",
      "‚úì gemini modeli ba≈ülatƒ±ldƒ±: models/gemini-2.5-pro\n",
      "  Rate limit: Her istek arasƒ± 20.0 saniye bekleme\n",
      "  [DEBUG] Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'This model is currently exp\n",
      "  ‚ö†Ô∏è Retrying in 20.0s...\n",
      "  [DEBUG] Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'This model is currently exp\n",
      "  ‚ö†Ô∏è Retrying in 40.0s...\n",
      "‚úÖ 0.000 (n=5)\n",
      "\n",
      "üîç Sample Mismatches:\n",
      "   FN: Thyroid abnormality @ ['thyroid gland']\n",
      "   FN: Effusion @ ['pericardium']\n",
      "   FP: Thyroid abnormality @ ['thyroid gland']\n",
      "   FP: Calcification @ ['LAD', 'RCA']\n",
      "\n",
      "  üíæ Saved: result_sample0.3.json\n",
      "\n",
      "üìÑ Summary saved: data_report/0/entity_level_results/SUMMARY.txt\n",
      "\n",
      "======================================================================\n",
      "üìä COMPREHENSIVE COMPARISON TABLE\n",
      "======================================================================\n",
      "Sample          Structural   PubMedBERT   S-PubMedB    LLM       \n",
      "----------------------------------------------------------------------\n",
      "0.0             0.900        0.998        0.992        0.920     \n",
      "0.1             0.300        0.982        0.931        0.040     \n",
      "0.2             0.733        0.993        0.972        0.540     \n",
      "0.3             0.250        0.986        0.945        0.000     \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìã Interpretation:\n",
      "   ‚Ä¢ Structural: Entity-level F1 (exact matching)\n",
      "   ‚Ä¢ PubMedBERT: Semantic similarity (medical context)\n",
      "   ‚Ä¢ S-PubMedB:  Sentence-level semantic similarity\n",
      "   ‚Ä¢ LLM:        Gemini clinical assessment\n",
      "\n",
      "üí° Note: If Semantic > Structural, model captures meaning\n",
      "         but makes exact matching errors (synonyms, etc.)\n",
      "\n",
      "‚úÖ DONE!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: ENTITY-LEVEL EVALUATION WITH SEMANTIC MATCHING + ERROR DETECTION\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Entity-level evaluation with SapBERT semantic matching\n",
    "+ Structural Error Detection (Merged/Split entities)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT EVALUATORS\n",
    "# ============================================================================\n",
    "\n",
    "from entity_level_evaluator import EntityLevelEvaluator, SemanticMedicalMatcher\n",
    "from llm_evaluator import LLMEvaluator\n",
    "from multi_embedding_evaluator import EmbeddingEvaluator\n",
    "\n",
    "# ============================================================================\n",
    "# üÜï YENƒ∞ EKLENEN: Structural Error Detection Fonksiyonlarƒ±\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ‚≠ê USER CONFIGURATION (Aynƒ± kalƒ±yor)\n",
    "# ============================================================================\n",
    "class UserConfig:\n",
    "    \"\"\"\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    üìù EDIT THESE SETTINGS TO CONTROL WHICH MODELS TO USE\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \"\"\"\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # üîë API KEYS - Add your keys here\n",
    "    # ----------------------------------------------------------------\n",
    "    API_KEYS = {\n",
    "        \"gemini\": \"AIzaSyCKbu90y_CTrfEc5fEKuWTYha2FvwfySVA\",\n",
    "        #\"gemma\": \"\",           \n",
    "        #\"glm\": \"\",             \n",
    "        #\"deepseek\": \"\",        \n",
    "    }\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # ü§ñ LLM MODELS TO USE\n",
    "    # ----------------------------------------------------------------\n",
    "    SELECTED_LLM_MODELS = [\n",
    "        \"gemini_pro\",          # ‚úÖ USING THIS\n",
    "    ]\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # üß† EMBEDDING MODELS TO USE\n",
    "    # ----------------------------------------------------------------\n",
    "    SELECTED_EMBEDDING_MODELS = [\n",
    "        \"pubmedbert\",           \n",
    "        \"s_pubmedbert\",         \n",
    "        \"general_baseline\",   \n",
    "        \"neuml_pubmedbert\",   \n",
    "    ]\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # üìÅ DATA CONFIGURATION\n",
    "    # ----------------------------------------------------------------\n",
    "    DATA_DIR = \"./data_report/0/\"\n",
    "    GT_FILENAME = \"gt0.json\"\n",
    "    OUTPUT_DIR = None  # Auto-generated if None\n",
    "    \n",
    "    # Semantic matching threshold\n",
    "    MATCH_THRESHOLD = 0.6  # Minimum score to consider entities matched\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# END OF USER CONFIGURATION\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "def detect_entity_structural_errors(matches: List[Dict], gt_entities: List[Dict], pred_entities: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Yapƒ±sal hatalarƒ± tespit et:\n",
    "    1. MERGED: 2+ GT entity -> 1 Pred entity (√∂rnekteki Cardiomegaly+Effusion durumu)\n",
    "    2. SPLIT: 1 GT entity -> 2+ Pred entity  \n",
    "    3. DEGREE_MIXUP: Uyumsuz degree kombinasyonu ([\"mild\", \"trace\"])\n",
    "    \"\"\"\n",
    "    \n",
    "    errors = {\n",
    "        'merged_entities': [],      \n",
    "        'split_entities': [],       \n",
    "        'degree_mixups': [],        \n",
    "        'contradictions': []        \n",
    "    }\n",
    "    \n",
    "    # Helper: Pred entity index'ini bul\n",
    "    def get_pred_idx(pred_ent):\n",
    "        try:\n",
    "            return pred_entities.index(pred_ent)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    # 1. MERGED ENTITY Detection\n",
    "    pred_to_gt = defaultdict(list)\n",
    "    \n",
    "    for match in matches:\n",
    "        if match['match_type'] == 'matched' and match.get('pred_entity'):\n",
    "            pred_idx = get_pred_idx(match['pred_entity'])\n",
    "            if pred_idx is not None:\n",
    "                pred_to_gt[pred_idx].append({\n",
    "                    'gt_entity': match['gt_entity'],\n",
    "                    'match_score': match.get('match_score', 0)\n",
    "                })\n",
    "    \n",
    "    # Birle≈ütirilmi≈ü entity'leri tespit et\n",
    "    for pred_idx, gt_matches in pred_to_gt.items():\n",
    "        if len(gt_matches) > 1:\n",
    "            pred_ent = pred_entities[pred_idx]\n",
    "            pred_finding = pred_ent.get('general_finding', 'Unknown')\n",
    "            pred_degree = pred_ent.get('degree', [])\n",
    "            \n",
    "            # GT bilgilerini topla\n",
    "            gt_findings = []\n",
    "            for gm in gt_matches:\n",
    "                gt_ent = gm['gt_entity']\n",
    "                gt_findings.append({\n",
    "                    'finding': gt_ent.get('general_finding'),\n",
    "                    'specific': gt_ent.get('specific_finding'),\n",
    "                    'degree': gt_ent.get('degree', []),\n",
    "                    'location': gt_ent.get('location', [])\n",
    "                })\n",
    "            \n",
    "            error_info = {\n",
    "                'type': 'MERGED_ENTITY',\n",
    "                'severity': 'HIGH',\n",
    "                'description': f\"{len(gt_matches)} farklƒ± GT entity tek entity'de birle≈ütirilmi≈ü\",\n",
    "                'gt_entities': gt_findings,\n",
    "                'pred_entity': {\n",
    "                    'finding': pred_finding,\n",
    "                    'degree': pred_degree,\n",
    "                    'location': pred_ent.get('location', [])\n",
    "                },\n",
    "                'impact': f\"{len(gt_matches)-1} entity kayƒ±p (FN)\"\n",
    "            }\n",
    "            errors['merged_entities'].append(error_info)\n",
    "            \n",
    "            # Degree mixup kontrol√º\n",
    "            if len(pred_degree) > 1:\n",
    "                # Uyumsuz degree kontrol√º\n",
    "                degree_set = set(str(d).lower() for d in pred_degree)\n",
    "                exclusive = {'mild', 'moderate', 'severe', 'trace', 'small', 'large'}\n",
    "                if len(degree_set & exclusive) > 1:\n",
    "                    errors['degree_mixups'].append({\n",
    "                        'finding': pred_finding,\n",
    "                        'degrees': pred_degree,\n",
    "                        'source_entities': [g['finding'] for g in gt_findings],\n",
    "                        'reason': \"Farklƒ± entity'lerden gelen degree'ler birle≈ütirilmi≈ü\"\n",
    "                    })\n",
    "    \n",
    "    # 2. SPLIT ENTITY Detection\n",
    "    gt_to_pred = defaultdict(list)\n",
    "    for i, match in enumerate(matches):\n",
    "        if match['match_type'] == 'matched' and match.get('gt_entity'):\n",
    "            try:\n",
    "                gt_idx = gt_entities.index(match['gt_entity'])\n",
    "                gt_to_pred[gt_idx].append(match['pred_entity'])\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    for gt_idx, pred_ents in gt_to_pred.items():\n",
    "        if len(pred_ents) > 1:\n",
    "            gt_ent = gt_entities[gt_idx]\n",
    "            errors['split_entities'].append({\n",
    "                'type': 'SPLIT_ENTITY',\n",
    "                'severity': 'MEDIUM',\n",
    "                'gt_entity': {\n",
    "                    'finding': gt_ent.get('general_finding'),\n",
    "                    'degree': gt_ent.get('degree')\n",
    "                },\n",
    "                'split_into': [p.get('general_finding') for p in pred_ents],\n",
    "                'count': len(pred_ents)\n",
    "            })\n",
    "    \n",
    "    # 3. CONTRADICTION Detection\n",
    "    for match in matches:\n",
    "        if match['match_type'] == 'matched':\n",
    "            gt_ent = match['gt_entity']\n",
    "            pred_ent = match['pred_entity']\n",
    "            \n",
    "            gt_pres = str(gt_ent.get('finding_presence', '')).lower()\n",
    "            pred_pres = str(pred_ent.get('finding_presence', '')).lower()\n",
    "            \n",
    "            contradictions = [('present', 'absent'), ('absent', 'present'), ('normal', 'abnormal')]\n",
    "            if (gt_pres, pred_pres) in contradictions:\n",
    "                errors['contradictions'].append({\n",
    "                    'finding': gt_ent.get('general_finding'),\n",
    "                    'gt_presence': gt_pres,\n",
    "                    'pred_presence': pred_pres\n",
    "                })\n",
    "    \n",
    "    return errors\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL CONFIGURATIONS\n",
    "# ============================================================================\n",
    "\n",
    "EMBEDDING_MODELS = {\n",
    "    \"pubmedbert\": {\n",
    "        \"name\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        \"description\": \"PubMed trained\"\n",
    "    },\n",
    "    \"s_pubmedbert\": {\n",
    "        \"name\": \"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        \"description\": \"Sentence-level\"\n",
    "    },\n",
    "    \"general_baseline\": {\n",
    "        \"name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"description\": \"General baseline\"\n",
    "    },\n",
    "    \"neuml_pubmedbert\": {\n",
    "        \"name\": \"NeuML/pubmedbert-base-embeddings\",\n",
    "        \"description\": \"NeuML embeddings\"\n",
    "    }\n",
    "}\n",
    "\n",
    "LLM_MODELS = {\n",
    "    #\"gemini_flash\": {\"type\": \"gemini\", \"name\": \"models/gemini-2.5-flash\"},\n",
    "    \"gemini_pro\": {\"type\": \"gemini\", \"name\": \"models/gemini-2.5-pro\"},\n",
    "    #\"gemma\": {\"type\": \"gemma\", \"name\": \"gemma-3-27b-it\"},\n",
    "    #\"glm\": {\"type\": \"glm\", \"name\": \"glm-4-flash\"},\n",
    "    #\"deepseek\": {\"type\": \"deepseek\", \"name\": \"deepseek-chat\"}\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION & DISPLAY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTITY-LEVEL EVALUATION v2.0 (Semantic Matching)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Validate selections\n",
    "valid_llm_keys = set(LLM_MODELS.keys())\n",
    "invalid_llm = [k for k in UserConfig.SELECTED_LLM_MODELS if k not in valid_llm_keys]\n",
    "if invalid_llm:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Invalid LLM models: {invalid_llm}\")\n",
    "    UserConfig.SELECTED_LLM_MODELS = [k for k in UserConfig.SELECTED_LLM_MODELS if k in valid_llm_keys]\n",
    "\n",
    "valid_emb_keys = set(EMBEDDING_MODELS.keys())\n",
    "invalid_emb = [k for k in UserConfig.SELECTED_EMBEDDING_MODELS if k not in valid_emb_keys]\n",
    "if invalid_emb:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Invalid embedding models: {invalid_emb}\")\n",
    "    UserConfig.SELECTED_EMBEDDING_MODELS = [k for k in UserConfig.SELECTED_EMBEDDING_MODELS if k in valid_emb_keys]\n",
    "\n",
    "print(f\"\\nüìÇ Data Directory: {UserConfig.DATA_DIR}\")\n",
    "print(f\"üìÑ Ground Truth: {UserConfig.GT_FILENAME}\")\n",
    "print(f\"\\nü§ñ Selected LLM Models ({len(UserConfig.SELECTED_LLM_MODELS)}):\")\n",
    "for llm in UserConfig.SELECTED_LLM_MODELS:\n",
    "    llm_info = LLM_MODELS[llm]\n",
    "    has_key = \"‚úÖ\" if UserConfig.API_KEYS.get(llm_info['type']) else \"‚ùå\"\n",
    "    print(f\"   {has_key} {llm}: {llm_info['name']}\")\n",
    "\n",
    "print(f\"\\nüß† Selected Embedding Models ({len(UserConfig.SELECTED_EMBEDDING_MODELS)}):\")\n",
    "for emb in UserConfig.SELECTED_EMBEDDING_MODELS:\n",
    "    emb_info = EMBEDDING_MODELS[emb]\n",
    "    print(f\"   ‚úÖ {emb}: {emb_info['name']}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION CLASS\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class EntityLevelReportEvaluator:\n",
    "    \n",
    "    def __init__(self, api_keys: dict, match_threshold: float = 0.6):\n",
    "        self.api_keys = api_keys\n",
    "        self.match_threshold = match_threshold\n",
    "        \n",
    "        print(\"\\nüîß Initializing Semantic Medical Matcher...\")\n",
    "        self.semantic_matcher = SemanticMedicalMatcher(use_embeddings=True)\n",
    "        self.entity_evaluator = EntityLevelEvaluator(\n",
    "            use_semantic_matching=True,\n",
    "            use_llm_for_borderline=False,\n",
    "            llm_evaluator=None\n",
    "        )\n",
    "        self.embedding_cache = {}\n",
    "        print(\"üß† Pre-loading embedding models...\")\n",
    "        for emb_key, emb_config in EMBEDDING_MODELS.items():\n",
    "            try:\n",
    "                emb_eval = EmbeddingEvaluator(emb_config['name'])\n",
    "                if emb_eval.model:\n",
    "                    self.embedding_cache[emb_key] = emb_eval\n",
    "                    print(f\"   ‚úÖ Cached: {emb_key}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Failed: {emb_key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error loading {emb_key}: {e}\")\n",
    "        \n",
    "        print(\"‚úÖ Evaluator ready!\")\n",
    "    \n",
    "    def evaluate_single(self, gt_path: str, pred_path: str,\n",
    "                       embedding_models: List[str] = None,\n",
    "                       llm_models: List[str] = None) -> Dict:\n",
    "        \n",
    "        with open(gt_path, 'r') as f:\n",
    "            gt_schema = json.load(f)\n",
    "        with open(pred_path, 'r') as f:\n",
    "            pred_schema = json.load(f)\n",
    "        \n",
    "        result = {\n",
    "            'gt_file': Path(gt_path).name,\n",
    "            'pred_file': Path(pred_path).name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        # Report check\n",
    "        if gt_schema.get('report', '').strip() != pred_schema.get('report', '').strip():\n",
    "            return {**result, 'status': 'error', 'error': 'Reports do not match'}\n",
    "        \n",
    "        # Entity extraction\n",
    "        gt_entities = self.entity_evaluator.flatten_entities(gt_schema)\n",
    "        pred_entities = self.entity_evaluator.flatten_entities(pred_schema)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üìÅ Evaluating: {Path(pred_path).name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"üìä Entity Count: GT={len(gt_entities)} | Pred={len(pred_entities)}\")\n",
    "        \n",
    "        # Matching\n",
    "        matches = self.entity_evaluator.match_entities(\n",
    "            gt_entities, pred_entities, report_text=gt_schema.get('report', '')\n",
    "        )\n",
    "        \n",
    "        # Metrics\n",
    "        base_metrics = self.entity_evaluator.compute_metrics(matches)\n",
    "        \n",
    "        # üÜï Structural error detection (detailed)\n",
    "        structural_errors = detect_entity_structural_errors(matches, gt_entities, pred_entities)\n",
    "        \n",
    "        # üÜï Per-field error analysis\n",
    "        field_errors = self._analyze_field_errors(matches)\n",
    "        \n",
    "        result['entity_metrics'] = {\n",
    "            'total_gt': len(gt_entities),\n",
    "            'total_pred': len(pred_entities),\n",
    "            'true_positives': base_metrics['true_positives'],\n",
    "            'false_positives': base_metrics['false_positives'],\n",
    "            'false_negatives': base_metrics['false_negatives'],\n",
    "            'precision': base_metrics['precision'],\n",
    "            'recall': base_metrics['recall'],\n",
    "            'f1_score': base_metrics['f1_score'],\n",
    "            'avg_match_quality': base_metrics['avg_match_quality'],\n",
    "            'contradictions': base_metrics['contradiction_count'],\n",
    "            'field_wise_accuracy': base_metrics.get('field_wise_accuracy', {}),\n",
    "            'structural_errors': structural_errors,  # üÜï Detailed errors\n",
    "            'field_error_analysis': field_errors     # üÜï Per-field breakdown\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìà STRUCTURAL (Entity-Level):\")\n",
    "        print(f\"   Precision: {base_metrics['precision']:.3f}\")\n",
    "        print(f\"   Recall:    {base_metrics['recall']:.3f}\")\n",
    "        print(f\"   F1-Score:  {base_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   TP: {base_metrics['true_positives']} | FP: {base_metrics['false_positives']} | FN: {base_metrics['false_negatives']}\")\n",
    "        \n",
    "        if structural_errors['merged_entities']:\n",
    "            print(f\"   üî¥ Merged Entities: {len(structural_errors['merged_entities'])}\")\n",
    "        if structural_errors['split_entities']:\n",
    "            print(f\"   üü° Split Entities: {len(structural_errors['split_entities'])}\")\n",
    "        if structural_errors['contradictions']:\n",
    "            print(f\"   ‚ö´ Contradictions: {len(structural_errors['contradictions'])}\")\n",
    "        \n",
    "        # ==========================================\n",
    "        # üÜï EMBEDDING EVALUATION (Cached + FN/FP Analysis)\n",
    "        # ==========================================\n",
    "        result['embedding_scores'] = {}\n",
    "        result['error_analysis'] = {\n",
    "            'fp_semantic_scores': {},  # False Positive semantic similarities\n",
    "            'fn_semantic_scores': {}   # False Negative semantic similarities\n",
    "        }\n",
    "        \n",
    "        if embedding_models:\n",
    "            print(f\"\\nüîç SEMANTIC (Embedding) Evaluation:\")\n",
    "            \n",
    "            for emb_key in embedding_models:\n",
    "                if emb_key not in self.embedding_cache:\n",
    "                    continue\n",
    "                \n",
    "                emb_eval = self.embedding_cache[emb_key]\n",
    "                print(f\"   Using cached {emb_key}...\")\n",
    "                \n",
    "                # 1. Matched pairs (TP)\n",
    "                tp_scores = []\n",
    "                for match in matches:\n",
    "                    if match['match_type'] == 'matched':\n",
    "                        try:\n",
    "                            sim = emb_eval.compute_similarity(\n",
    "                                {'output': [match['gt_entity']]}, \n",
    "                                {'output': [match['pred_entity']]}\n",
    "                            )\n",
    "                            tp_scores.append(float(sim))\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                # 2. False Positive Analysis (Pred has it, GT doesn't)\n",
    "                fp_scores = []\n",
    "                for match in matches:\n",
    "                    if match['match_type'] == 'false_positive' and match['pred_entity']:\n",
    "                        # Find most similar GT entity (to see if it's a misalignment)\n",
    "                        best_sim = 0\n",
    "                        for gt_ent in gt_entities:\n",
    "                            try:\n",
    "                                sim = emb_eval.compute_similarity(\n",
    "                                    {'output': [gt_ent]}, \n",
    "                                    {'output': [match['pred_entity']]}\n",
    "                                )\n",
    "                                best_sim = max(best_sim, float(sim))\n",
    "                            except:\n",
    "                                continue\n",
    "                        fp_scores.append(best_sim)\n",
    "                \n",
    "                # 3. False Negative Analysis (GT has it, Pred doesn't)\n",
    "                fn_scores = []\n",
    "                for match in matches:\n",
    "                    if match['match_type'] == 'false_negative' and match['gt_entity']:\n",
    "                        # Find most similar Pred entity\n",
    "                        best_sim = 0\n",
    "                        for pred_ent in pred_entities:\n",
    "                            try:\n",
    "                                sim = emb_eval.compute_similarity(\n",
    "                                    {'output': [match['gt_entity']]}, \n",
    "                                    {'output': [pred_ent]}\n",
    "                                )\n",
    "                                best_sim = max(best_sim, float(sim))\n",
    "                            except:\n",
    "                                continue\n",
    "                        fn_scores.append(best_sim)\n",
    "                \n",
    "                # Store results\n",
    "                if tp_scores:\n",
    "                    result['embedding_scores'][emb_key] = {\n",
    "                        'mean': float(np.mean(tp_scores)),\n",
    "                        'std': float(np.std(tp_scores)),\n",
    "                        'count': len(tp_scores),\n",
    "                        'type': 'true_positives'\n",
    "                    }\n",
    "                    print(f\"      ‚úÖ TP: {np.mean(tp_scores):.3f} (n={len(tp_scores)})\")\n",
    "                \n",
    "                if fp_scores:\n",
    "                    result['error_analysis']['fp_semantic_scores'][emb_key] = {\n",
    "                        'mean': float(np.mean(fp_scores)),\n",
    "                        'std': float(np.std(fp_scores)),\n",
    "                        'count': len(fp_scores)\n",
    "                    }\n",
    "                    print(f\"      ‚ö†Ô∏è  FP (sem): {np.mean(fp_scores):.3f} (n={len(fp_scores)})\")\n",
    "                \n",
    "                if fn_scores:\n",
    "                    result['error_analysis']['fn_semantic_scores'][emb_key] = {\n",
    "                        'mean': float(np.mean(fn_scores)),\n",
    "                        'std': float(np.std(fn_scores)),\n",
    "                        'count': len(fn_scores)\n",
    "                    }\n",
    "                    print(f\"      ‚ö†Ô∏è  FN (sem): {np.mean(fn_scores):.3f} (n={len(fn_scores)})\")\n",
    "        \n",
    "        # ==========================================\n",
    "        # LLM EVALUATION (Fixed)\n",
    "        # ==========================================\n",
    "        result['llm_scores'] = {}\n",
    "        \n",
    "        if llm_models:\n",
    "            print(f\"\\nü§ñ LLM Evaluation:\")\n",
    "            \n",
    "            for llm_key in llm_models:\n",
    "                if llm_key not in LLM_MODELS:\n",
    "                    continue\n",
    "                \n",
    "                llm_config = LLM_MODELS[llm_key]\n",
    "                api_key = self.api_keys.get(llm_config['type'])\n",
    "                \n",
    "                if not api_key:\n",
    "                    print(f\"   ‚ö†Ô∏è  No API key for {llm_key}\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"   Testing {llm_key}...\", end=\" \")\n",
    "                \n",
    "                try:\n",
    "                    llm_eval = LLMEvaluator(\n",
    "                        model_type=llm_config['type'],\n",
    "                        model_name=llm_config['name'],\n",
    "                        api_key=api_key\n",
    "                    )\n",
    "                    \n",
    "                    # Evaluate matched pairs with rate limiting awareness\n",
    "                    llm_scores = []\n",
    "                    matched_pairs = [m for m in matches if m['match_type'] == 'matched'][:5]\n",
    "                    \n",
    "                    for i, match in enumerate(matched_pairs):\n",
    "                        try:\n",
    "                            llm_result = llm_eval.evaluate_schema_pair(\n",
    "                                {'output': [match['gt_entity']], 'input': ''},\n",
    "                                {'output': [match['pred_entity']], 'input': ''},\n",
    "                                gt_schema.get('report', '')[:500]\n",
    "                            )\n",
    "                            score = llm_result.get('similarity_score', 0)\n",
    "                            if score is not None:\n",
    "                                llm_scores.append(float(score))\n",
    "                        except Exception as e:\n",
    "                            print(f\"\\n      ‚ö†Ô∏è  Pair {i+1} failed: {str(e)[:50]}\")\n",
    "                            continue\n",
    "                    \n",
    "                    if llm_scores:\n",
    "                        avg_llm = np.mean(llm_scores)\n",
    "                        result['llm_scores'][llm_key] = {\n",
    "                            'mean': float(avg_llm),\n",
    "                            'count': len(llm_scores)\n",
    "                        }\n",
    "                        print(f\"‚úÖ {avg_llm:.3f} (n={len(llm_scores)})\")\n",
    "                    else:\n",
    "                        print(\"‚ùå No scores\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Failed: {str(e)[:50]}\")\n",
    "        \n",
    "        # Sample mismatches\n",
    "        print(f\"\\nüîç Sample Mismatches:\")\n",
    "        fp_shown = fn_shown = 0\n",
    "        for match in matches:\n",
    "            if match['match_type'] == 'false_positive' and fp_shown < 2:\n",
    "                ent = match['pred_entity']\n",
    "                print(f\"   FP: {ent.get('general_finding')} @ {ent.get('location')}\")\n",
    "                fp_shown += 1\n",
    "            if match['match_type'] == 'false_negative' and fn_shown < 2:\n",
    "                ent = match['gt_entity']\n",
    "                print(f\"   FN: {ent.get('general_finding')} @ {ent.get('location')}\")\n",
    "                fn_shown += 1\n",
    "        \n",
    "        result['detailed_matches'] = matches\n",
    "        return result\n",
    "    \n",
    "    def _analyze_field_errors(self, matches: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Per-field error analysis: which fields cause most failures?\n",
    "        \"\"\"\n",
    "        field_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'errors': []})\n",
    "        \n",
    "        for match in matches:\n",
    "            if match['match_type'] != 'matched':\n",
    "                continue\n",
    "            \n",
    "            gt_ent = match['gt_entity']\n",
    "            pred_ent = match['pred_entity']\n",
    "            \n",
    "            # Check each field\n",
    "            fields = ['general_finding', 'specific_finding', 'finding_presence', 'location', 'degree']\n",
    "            for field in fields:\n",
    "                gt_val = gt_ent.get(field)\n",
    "                pred_val = pred_ent.get(field) if pred_ent else None\n",
    "                \n",
    "                field_stats[field]['total'] += 1\n",
    "                \n",
    "                # Simple equality check (could be enhanced with semantic similarity)\n",
    "                is_match = self._values_match(gt_val, pred_val)\n",
    "                if is_match:\n",
    "                    field_stats[field]['correct'] += 1\n",
    "                else:\n",
    "                    field_stats[field]['errors'].append({\n",
    "                        'gt': str(gt_val)[:50],\n",
    "                        'pred': str(pred_val)[:50]\n",
    "                    })\n",
    "        \n",
    "        # Calculate accuracy per field\n",
    "        result = {}\n",
    "        for field, stats in field_stats.items():\n",
    "            if stats['total'] > 0:\n",
    "                result[field] = {\n",
    "                    'accuracy': stats['correct'] / stats['total'],\n",
    "                    'total': stats['total'],\n",
    "                    'errors': len(stats['errors']),\n",
    "                    'error_rate': (stats['total'] - stats['correct']) / stats['total']\n",
    "                }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _values_match(self, val1, val2) -> bool:\n",
    "        \"\"\"Check if two values match (handles lists and None)\"\"\"\n",
    "        if val1 is None and val2 is None:\n",
    "            return True\n",
    "        if val1 is None or val2 is None:\n",
    "            return False\n",
    "        \n",
    "        # Normalize lists\n",
    "        if isinstance(val1, list):\n",
    "            val1 = set(str(x).lower() for x in val1 if x)\n",
    "        else:\n",
    "            val1 = str(val1).lower()\n",
    "            \n",
    "        if isinstance(val2, list):\n",
    "            val2 = set(str(x).lower() for x in val2 if x)\n",
    "        else:\n",
    "            val2 = str(val2).lower()\n",
    "        \n",
    "        return val1 == val2\n",
    "    \n",
    "\n",
    "def bootstrap_confidence_interval(data: List[float], n_bootstrap: int = 1000, confidence: float = 0.95) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence interval for a metric\n",
    "    \"\"\"\n",
    "    # In bootstrap_confidence_interval function, add:\n",
    "    if len(data) < 2:\n",
    "        return (float(np.mean(data)), float(np.mean(data)))  # Return mean as both bounds\n",
    "    \n",
    "    bootstrap_means = []\n",
    "    n = len(data)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = np.random.choice(data, size=n, replace=True)\n",
    "        bootstrap_means.append(np.mean(sample))\n",
    "    \n",
    "    alpha = 1 - confidence\n",
    "    lower = np.percentile(bootstrap_means, 100 * alpha / 2)\n",
    "    upper = np.percentile(bootstrap_means, 100 * (1 - alpha / 2))\n",
    "    \n",
    "    return (float(lower), float(upper))\n",
    "\n",
    "def calculate_correlations(results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate correlations between Structural F1 and Semantic scores\n",
    "    \"\"\"\n",
    "    successful = [r for r in results if r.get('status') == 'success']\n",
    "    if len(successful) < 2:\n",
    "        return {}\n",
    "    \n",
    "    # Extract scores\n",
    "    structural_scores = [r['entity_metrics']['f1_score'] for r in successful]\n",
    "    \n",
    "    correlations = {}\n",
    "    \n",
    "    # Correlate with each embedding model\n",
    "    for emb_key in ['pubmedbert', 's_pubmedbert', 'general_baseline', 'neuml_pubmedbert']:\n",
    "        emb_scores = [r.get('embedding_scores', {}).get(emb_key, {}).get('mean', 0) for r in successful]\n",
    "        \n",
    "        if any(emb_scores):  # If we have scores\n",
    "            # Pearson correlation\n",
    "            if len(structural_scores) == len(emb_scores) and len(structural_scores) > 1:\n",
    "                corr_matrix = np.corrcoef(structural_scores, emb_scores)\n",
    "                correlations[f'structural_vs_{emb_key}'] = {\n",
    "                    'pearson_r': float(corr_matrix[0, 1]) if corr_matrix.shape == (2, 2) else 0.0,\n",
    "                    'structural_mean': float(np.mean(structural_scores)),\n",
    "                    'structural_std': float(np.std(structural_scores)),\n",
    "                    'semantic_mean': float(np.mean(emb_scores)),\n",
    "                    'semantic_std': float(np.std(emb_scores))\n",
    "                }\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "def analyze_structural_vs_semantic_gap(results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the gap between structural and semantic scores\n",
    "    \"\"\"\n",
    "    gaps = []\n",
    "    for r in results:\n",
    "        if r.get('status') != 'success':\n",
    "            continue\n",
    "        \n",
    "        struct = r['entity_metrics']['f1_score']\n",
    "        sem = r.get('embedding_scores', {}).get('pubmedbert', {}).get('mean', 0)\n",
    "        \n",
    "        if sem > 0:\n",
    "            gap = sem - struct\n",
    "            normalized_gap = gap / struct if struct > 0 else 0\n",
    "            gaps.append({\n",
    "                'sample': r['pred_file'],\n",
    "                'structural': struct,\n",
    "                'semantic': sem,\n",
    "                'gap': gap,\n",
    "                'normalized_gap': normalized_gap,\n",
    "                'interpretation': 'Meaning captured but exact matching failed' if gap > 0.3 else 'Aligned'\n",
    "            })\n",
    "    \n",
    "    if not gaps:\n",
    "        return {}\n",
    "    \n",
    "    return {\n",
    "        'mean_gap': float(np.mean([g['gap'] for g in gaps])),\n",
    "        'mean_normalized_gap': float(np.mean([g['normalized_gap'] for g in gaps])),\n",
    "        'high_divergence_samples': [g for g in gaps if g['normalized_gap'] > 0.5],\n",
    "        'sample_count': len(gaps)\n",
    "    }\n",
    "\n",
    "def evaluate_directory(\n",
    "    data_dir: str = None,\n",
    "    gt_filename: str = None,\n",
    "    output_dir: str = None,\n",
    "    embedding_models: List[str] = None,\n",
    "    llm_models: List[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate all samples in directory\n",
    "    \"\"\"\n",
    "    # Defaults\n",
    "    data_dir = data_dir or UserConfig.DATA_DIR\n",
    "    gt_filename = gt_filename or UserConfig.GT_FILENAME\n",
    "    output_dir = output_dir or (Path(data_dir) / 'entity_level_results')\n",
    "    embedding_models = embedding_models or UserConfig.SELECTED_EMBEDDING_MODELS\n",
    "    llm_models = llm_models or UserConfig.SELECTED_LLM_MODELS\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    gt_path = data_path / gt_filename\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not gt_path.exists():\n",
    "        print(f\"‚ùå GT file not found: {gt_path}\")\n",
    "        return\n",
    "    \n",
    "    # Find test files\n",
    "    test_files = sorted(data_path.glob(\"sample*.json\"))\n",
    "    \n",
    "    if not test_files:\n",
    "        print(f\"‚ö†Ô∏è No test files found in {data_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BATCH EVALUATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Directory: {data_dir}\")\n",
    "    print(f\"GT: {gt_filename}\")\n",
    "    print(f\"Test files: {len(test_files)}\")\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = EntityLevelReportEvaluator(\n",
    "        api_keys=UserConfig.API_KEYS,\n",
    "        match_threshold=UserConfig.MATCH_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # Evaluate each file\n",
    "    all_results = []\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        result = evaluator.evaluate_single(\n",
    "            str(gt_path),\n",
    "            str(test_file),\n",
    "            embedding_models=embedding_models,\n",
    "            llm_models=llm_models\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save individual result\n",
    "        result_file = output_path / f\"result_{test_file.stem}.json\"\n",
    "        with open(result_file, 'w') as f:\n",
    "            # Don't save full matches to keep file size reasonable\n",
    "            result_summary = {k: v for k, v in result.items() if k != 'detailed_matches'}\n",
    "            json.dump(result_summary, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n  üíæ Saved: {result_file.name}\")\n",
    "    \n",
    "    # Generate summary\n",
    "    _generate_summary(all_results, output_path)\n",
    "    \n",
    "    # üÜï FINAL COMPARISON TABLE\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä COMPREHENSIVE COMPARISON TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Sample':<15} {'Structural':<12} {'PubMedBERT':<12} {'S-PubMedB':<12} {'LLM':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for r in all_results:\n",
    "        if r.get('status') != 'success':\n",
    "            print(f\"{r['pred_file']:<15} {'ERROR':<12}\")\n",
    "            continue\n",
    "        \n",
    "        sample = r['pred_file'].replace('sample', '').replace('.json', '')\n",
    "        struct_f1 = r['entity_metrics']['f1_score']\n",
    "        \n",
    "        # Get embedding scores (default to 0 if missing)\n",
    "        emb_pub = r.get('embedding_scores', {}).get('pubmedbert', {}).get('mean', 0)\n",
    "        emb_spub = r.get('embedding_scores', {}).get('s_pubmedbert', {}).get('mean', 0)\n",
    "        \n",
    "        # Get LLM score\n",
    "        llm_score = r.get('llm_scores', {}).get('gemini_pro', {}).get('mean', 0)\n",
    "        \n",
    "        print(f\"{sample:<15} {struct_f1:<12.3f} {emb_pub:<12.3f} {emb_spub:<12.3f} {llm_score:<10.3f}\")\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Interpretation guide\n",
    "    print(f\"\\nüìã Interpretation:\")\n",
    "    print(f\"   ‚Ä¢ Structural: Entity-level F1 (exact matching)\")\n",
    "    print(f\"   ‚Ä¢ PubMedBERT: Semantic similarity (medical context)\")\n",
    "    print(f\"   ‚Ä¢ S-PubMedB:  Sentence-level semantic similarity\")\n",
    "    print(f\"   ‚Ä¢ LLM:        Gemini clinical assessment\")\n",
    "    print(f\"\\nüí° Note: If Semantic > Structural, model captures meaning\")\n",
    "    print(f\"         but makes exact matching errors (synonyms, etc.)\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def _generate_summary(results: List[Dict], output_dir: Path):\n",
    "    \"\"\"Generate comprehensive summary report with statistics\"\"\"\n",
    "    summary_path = output_dir / \"SUMMARY.txt\"\n",
    "    \n",
    "    successful = [r for r in results if r.get('status') == 'success']\n",
    "    \n",
    "    # üÜï Statistical calculations\n",
    "    f1_scores = [r['entity_metrics']['f1_score'] for r in successful]\n",
    "    f1_ci = bootstrap_confidence_interval(f1_scores) if f1_scores else (0, 0)\n",
    "    correlations = calculate_correlations(results)\n",
    "    gap_analysis = analyze_structural_vs_semantic_gap(results)\n",
    "    \n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        f.write(\"ENTITY-LEVEL EVALUATION SUMMARY v2.1 (Professional)\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
    "        f.write(f\"Total samples: {len(results)}\\n\")\n",
    "        f.write(f\"Successful: {len(successful)}\\n\")\n",
    "        f.write(f\"Failed: {len(results) - len(successful)}\\n\\n\")\n",
    "        \n",
    "        # üÜï Confidence Intervals\n",
    "        if f1_scores:\n",
    "            f.write(\"CONFIDENCE INTERVALS (95% Bootstrap):\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(f\"Structural F1: {np.mean(f1_scores):.3f} [{f1_ci[0]:.3f}, {f1_ci[1]:.3f}]\\n\\n\")\n",
    "        \n",
    "        # Per-field error analysis\n",
    "        f.write(\"PER-FIELD ERROR ANALYSIS:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        all_field_errors = defaultdict(lambda: {'total': 0, 'errors': 0})\n",
    "        for r in successful:\n",
    "            field_analysis = r['entity_metrics'].get('field_error_analysis', {})\n",
    "            for field, stats in field_analysis.items():\n",
    "                all_field_errors[field]['total'] += stats.get('total', 0)\n",
    "                all_field_errors[field]['errors'] += stats.get('errors', 0)\n",
    "        \n",
    "        for field, stats in sorted(all_field_errors.items(), key=lambda x: x[1]['errors'], reverse=True):\n",
    "            error_rate = stats['errors'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "            f.write(f\"  {field:25s}: {error_rate:5.1f}% error ({stats['errors']}/{stats['total']})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Per-sample detailed results\n",
    "        f.write(\"PER-SAMPLE RESULTS:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        \n",
    "        for result in results:\n",
    "            if result.get('status') == 'error':\n",
    "                f.write(f\"\\n{result['pred_file']}: ‚ùå ERROR - {result['error']}\\n\")\n",
    "                continue\n",
    "            \n",
    "            metrics = result['entity_metrics']\n",
    "            f.write(f\"\\n{result['pred_file']}:\\n\")\n",
    "            f.write(f\"  üìä STRUCTURAL (Entity-Level):\\n\")\n",
    "            f.write(f\"    Entities: GT={metrics['total_gt']}, Pred={metrics['total_pred']}\\n\")\n",
    "            f.write(f\"    Precision: {metrics['precision']:.3f}\\n\")\n",
    "            f.write(f\"    Recall:    {metrics['recall']:.3f}\\n\")\n",
    "            f.write(f\"    F1-Score:  {metrics['f1_score']:.3f}\\n\")\n",
    "            f.write(f\"    TP: {metrics['true_positives']}, FP: {metrics['false_positives']}, FN: {metrics['false_negatives']}\\n\")\n",
    "            \n",
    "            # Field-wise accuracy\n",
    "            if metrics.get('field_wise_accuracy'):\n",
    "                f.write(f\"  üî¨ Field-wise Accuracy:\\n\")\n",
    "                for field, acc in metrics['field_wise_accuracy'].items():\n",
    "                    f.write(f\"    {field}: {acc:.3f}\\n\")\n",
    "            \n",
    "            # Embedding scores\n",
    "            if result.get('embedding_scores'):\n",
    "                f.write(f\"  üîç SEMANTIC (Embeddings):\\n\")\n",
    "                for emb_name, emb_data in result['embedding_scores'].items():\n",
    "                    f.write(f\"    {emb_name}: {emb_data['mean']:.3f} (n={emb_data['count']})\\n\")\n",
    "                \n",
    "                # üÜï FN/FP Semantic scores\n",
    "                if result.get('error_analysis'):\n",
    "                    f.write(f\"  ‚ö†Ô∏è  Error Semantic Analysis:\\n\")\n",
    "                    for emb_name, fp_data in result['error_analysis'].get('fp_semantic_scores', {}).items():\n",
    "                        f.write(f\"    FP {emb_name}: {fp_data['mean']:.3f} (high=missed alignment)\\n\")\n",
    "            \n",
    "            # LLM scores\n",
    "            if result.get('llm_scores'):\n",
    "                f.write(f\"  ü§ñ LLM:\\n\")\n",
    "                for llm_name, llm_data in result['llm_scores'].items():\n",
    "                    f.write(f\"    {llm_name}: {llm_data['mean']:.3f}\\n\")\n",
    "            \n",
    "            # Structural errors\n",
    "            s_errors = metrics.get('structural_errors', {})\n",
    "            if s_errors.get('merged_entities'):\n",
    "                f.write(f\"  üî¥ Merged Entities: {len(s_errors['merged_entities'])}\\n\")\n",
    "                for me in s_errors['merged_entities'][:2]:  # Show first 2\n",
    "                    f.write(f\"      - {me['description']}\\n\")\n",
    "            if s_errors.get('contradictions'):\n",
    "                f.write(f\"  ‚ö´ Contradictions: {len(s_errors['contradictions'])}\\n\")\n",
    "        \n",
    "        # Comparison Table\n",
    "        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        f.write(\"COMPARISON TABLE (Structural vs Semantic vs LLM)\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        f.write(f\"{'Sample':<15} {'Structural':<12} {'PubMedBERT':<12} {'S-PubMedB':<12} {'LLM':<10}\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        \n",
    "        for r in results:\n",
    "            if r.get('status') != 'success':\n",
    "                f.write(f\"{r['pred_file']:<15} {'ERROR':<12}\\n\")\n",
    "                continue\n",
    "            \n",
    "            sample_name = r['pred_file'].replace('sample', '').replace('.json', '')\n",
    "            struct_f1 = r['entity_metrics']['f1_score']\n",
    "            emb_pub = r.get('embedding_scores', {}).get('pubmedbert', {}).get('mean', 0)\n",
    "            emb_spub = r.get('embedding_scores', {}).get('s_pubmedbert', {}).get('mean', 0)\n",
    "            llm_score = r.get('llm_scores', {}).get('gemini_pro', {}).get('mean', 0)\n",
    "            \n",
    "            f.write(f\"{sample_name:<15} {struct_f1:<12.3f} {emb_pub:<12.3f} {emb_spub:<12.3f} {llm_score:<10.3f}\\n\")\n",
    "        \n",
    "                # üÜï Structural Error Summary - WITH EXAMPLES\n",
    "        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        f.write(\"STRUCTURAL ERROR ANALYSIS\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        \n",
    "        # Collect actual error objects from all samples (not just counts)\n",
    "        all_merged = []\n",
    "        all_splits = []\n",
    "        all_contra = []\n",
    "        \n",
    "        for r in successful:\n",
    "            errors = r['entity_metrics'].get('structural_errors', {})\n",
    "            all_merged.extend(errors.get('merged_entities', []))\n",
    "            all_splits.extend(errors.get('split_entities', []))\n",
    "            all_contra.extend(errors.get('contradictions', []))\n",
    "        \n",
    "        total_merged = len(all_merged)\n",
    "        total_splits = len(all_splits)\n",
    "        total_contra = len(all_contra)\n",
    "        \n",
    "        f.write(f\"Total Merged Entities: {total_merged}\\n\")\n",
    "        f.write(f\"Total Split Entities: {total_splits}\\n\")\n",
    "        f.write(f\"Total Contradictions: {total_contra}\\n\")\n",
    "        \n",
    "    \n",
    "        # Show actual merged entity examples\n",
    "        if all_merged:\n",
    "            f.write(f\"\\nüî¥ MERGED ENTITY EXAMPLES:\\n\")\n",
    "            for i, me in enumerate(all_merged[:3], 1):\n",
    "                f.write(f\"  {i}. {me.get('description', 'N/A')}\\n\")\n",
    "                f.write(f\"     Severity: {me.get('severity', 'N/A')}\\n\")\n",
    "                f.write(f\"     Impact: {me.get('impact', 'N/A')}\\n\")\n",
    "                gt_ents = me.get('gt_entities', [])\n",
    "                if gt_ents:\n",
    "                    findings = [g.get('finding', 'Unknown') for g in gt_ents]\n",
    "                    f.write(f\"     Source entities: {', '.join(findings)}\\n\")\n",
    "                pred = me.get('pred_entity', {})\n",
    "                if pred:\n",
    "                    degrees = pred.get('degree', [])\n",
    "                    deg_str = ', '.join(degrees) if degrees else 'None'\n",
    "                    f.write(f\"     Merged into: {pred.get('finding', 'Unknown')} \"\n",
    "                           f\"(degrees: {deg_str})\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            if len(all_merged) > 3:\n",
    "                f.write(f\"     ... and {len(all_merged) - 3} more\\n\")\n",
    "        \n",
    "        # Show split entity examples  \n",
    "        if all_splits:\n",
    "            f.write(f\"\\nüü° SPLIT ENTITY EXAMPLES:\\n\")\n",
    "            for i, se in enumerate(all_splits[:3], 1):\n",
    "                gt_ent = se.get('gt_entity', {})\n",
    "                f.write(f\"  {i}. GT: {gt_ent.get('finding', 'Unknown')} \"\n",
    "                       f\"‚Üí Split into: {', '.join(se.get('split_into', []))}\\n\")\n",
    "                f.write(f\"     Severity: {se.get('severity', 'N/A')}\\n\")\n",
    "                f.write(f\"     Description: {se.get('description', 'N/A')}\\n\")\n",
    "        \n",
    "        # Show contradiction examples\n",
    "        if all_contra:\n",
    "            f.write(f\"\\n‚ö´ CONTRADICTION EXAMPLES:\\n\")\n",
    "            for i, co in enumerate(all_contra[:3], 1):\n",
    "                f.write(f\"  {i}. {co.get('finding', 'Unknown')}: \"\n",
    "                       f\"GT='{co.get('gt_presence', 'N/A')}' vs \"\n",
    "                       f\"Pred='{co.get('pred_presence', 'N/A')}'\\n\")\n",
    "        \n",
    "        # üÜï Correlation Analysis\n",
    "        if correlations:\n",
    "            f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "            f.write(\"CORRELATION ANALYSIS (Structural vs Semantic)\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            for corr_name, corr_data in correlations.items():\n",
    "                f.write(f\"{corr_name}:\\n\")\n",
    "                f.write(f\"  Pearson r: {corr_data['pearson_r']:.3f}\\n\")\n",
    "                f.write(f\"  Structural: {corr_data['structural_mean']:.3f} ¬± {corr_data['structural_std']:.3f}\\n\")\n",
    "                f.write(f\"  Semantic:   {corr_data['semantic_mean']:.3f} ¬± {corr_data['semantic_std']:.3f}\\n\")\n",
    "                if abs(corr_data['pearson_r']) < 0.3:\n",
    "                    f.write(f\"  ‚ö†Ô∏è  Low correlation: Metrics measure different aspects\\n\")\n",
    "                elif corr_data['pearson_r'] > 0.7:\n",
    "                    f.write(f\"  ‚úÖ High correlation: Metrics are aligned\\n\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        # üÜï Semantic vs Structural Gap Analysis\n",
    "        if gap_analysis:\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            f.write(\"SEMANTIC-STRUCTURAL DIVERGENCE ANALYSIS\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(f\"Mean Gap: {gap_analysis['mean_gap']:.3f}\\n\")\n",
    "            f.write(f\"Mean Normalized Gap: {gap_analysis['mean_normalized_gap']:.3f}\\n\")\n",
    "            if gap_analysis['high_divergence_samples']:\n",
    "                f.write(f\"\\nHigh Divergence Samples (semantic >> structural):\\n\")\n",
    "                for s in gap_analysis['high_divergence_samples'][:3]:\n",
    "                    f.write(f\"  - {s['sample']}: Structural={s['structural']:.3f}, Semantic={s['semantic']:.3f}\\n\")\n",
    "                    f.write(f\"    Interpretation: {s['interpretation']}\\n\")\n",
    "        \n",
    "        # Aggregate Statistics\n",
    "        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        f.write(\"AGGREGATE STATISTICS\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        \n",
    "        if successful:\n",
    "            avg_precision = np.mean([r['entity_metrics']['precision'] for r in successful])\n",
    "            avg_recall = np.mean([r['entity_metrics']['recall'] for r in successful])\n",
    "            avg_f1 = np.mean([r['entity_metrics']['f1_score'] for r in successful])\n",
    "            \n",
    "            f.write(f\"\\nAverage Structural Precision: {avg_precision:.3f}\\n\")\n",
    "            f.write(f\"Average Structural Recall:    {avg_recall:.3f}\\n\")\n",
    "            f.write(f\"Average Structural F1-Score:  {avg_f1:.3f}\\n\")\n",
    "            \n",
    "            # Average embeddings\n",
    "            for emb_name in ['pubmedbert', 's_pubmedbert', 'general_baseline', 'neuml_pubmedbert']:\n",
    "                scores = [r.get('embedding_scores', {}).get(emb_name, {}).get('mean', 0) \n",
    "                         for r in successful if r.get('embedding_scores', {}).get(emb_name)]\n",
    "                if scores:\n",
    "                    ci = bootstrap_confidence_interval(scores)\n",
    "                    f.write(f\"Average {emb_name}: {np.mean(scores):.3f} [{ci[0]:.3f}, {ci[1]:.3f}]\\n\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Summary saved: {summary_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ STARTING EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = evaluate_directory()\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ DONE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
