{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217dbb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Normalizing report-level schemas...\n",
      "\n",
      "======================================================================\n",
      "REPORT-LEVEL SCHEMA NORMALIZATION\n",
      "======================================================================\n",
      "\n",
      "Base directory: data_report\n",
      "Found 4 data directories: ['0', '1', '2', '3']\n",
      "\n",
      "======================================================================\n",
      "Processing: data_report/0\n",
      "======================================================================\n",
      "Found 5 JSON files\n",
      "  âœ… gt0.json (NEW format normalized)\n",
      "  âœ… sample0.0.json (NEW format normalized)\n",
      "  âœ… sample0.1.json (NEW format normalized)\n",
      "  âœ… sample0.2.json (NEW format normalized)\n",
      "  âœ… sample0.3.json (NEW format normalized)\n",
      "\n",
      "======================================================================\n",
      "Processing: data_report/1\n",
      "======================================================================\n",
      "Found 1 JSON files\n",
      "  âœ… gt1.json (NEW format normalized)\n",
      "\n",
      "======================================================================\n",
      "Processing: data_report/2\n",
      "======================================================================\n",
      "Found 1 JSON files\n",
      "  âœ… gt2.json (NEW format normalized)\n",
      "\n",
      "======================================================================\n",
      "Processing: data_report/3\n",
      "======================================================================\n",
      "Found 1 JSON files\n",
      "  âœ… gt3.json (NEW format normalized)\n",
      "\n",
      "======================================================================\n",
      "NORMALIZATION SUMMARY\n",
      "======================================================================\n",
      "Processed: 8\n",
      "Errors: 0\n",
      "\n",
      "âœ… ALL SCHEMAS NORMALIZED!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 0: ENHANCED JSON NORMALIZATION FOR REPORT-LEVEL SCHEMAS\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Normalizes both OLD and NEW schema formats:\n",
    "- OLD: {\"input\": \"...\", \"output\": [...]}\n",
    "- NEW: {\"report\": \"...\", \"inputs\": [{\"input\": \"...\", \"output\": [...]}]}\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "# ============================================================================\n",
    "# CANONICAL ENTITY FOR NEW FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "CANONICAL_ENTITY_NEW = {\n",
    "    \"general_finding\": None,\n",
    "    \"specific_finding\": None,\n",
    "    \"finding_presence\": \"unknown\",  # present/absent/uncertain\n",
    "    \"location\": [],\n",
    "    \"degree\": [],\n",
    "    \"measurement\": None,\n",
    "    \"comparison\": None\n",
    "}\n",
    "\n",
    "def normalize_entity_new_format(entity: dict) -> dict:\n",
    "    \"\"\"Normalize entity for NEW schema format\"\"\"\n",
    "    normalized = deepcopy(CANONICAL_ENTITY_NEW)\n",
    "    \n",
    "    for key in normalized:\n",
    "        if key not in entity:\n",
    "            continue\n",
    "        \n",
    "        value = entity[key]\n",
    "        \n",
    "        # \"None\" string â†’ None\n",
    "        if isinstance(value, str) and value.lower() == \"none\":\n",
    "            value = None\n",
    "        \n",
    "        # Empty string â†’ None\n",
    "        if isinstance(value, str) and value.strip() == \"\":\n",
    "            value = None\n",
    "        \n",
    "        # location & degree MUST be list\n",
    "        if key in (\"location\", \"degree\"):\n",
    "            if value is None:\n",
    "                value = []\n",
    "            elif isinstance(value, str):\n",
    "                value = [value] if value.strip() else []\n",
    "            elif not isinstance(value, list):\n",
    "                value = [str(value)]\n",
    "            # Remove \"None\" from lists\n",
    "            value = [v for v in value if str(v).lower() != \"none\"]\n",
    "        \n",
    "        # finding_presence normalize\n",
    "        if key == \"finding_presence\":\n",
    "            if value is None:\n",
    "                value = \"unknown\"\n",
    "            elif isinstance(value, str):\n",
    "                value = value.lower()\n",
    "                if value not in (\"present\", \"absent\", \"uncertain\"):\n",
    "                    value = \"unknown\"\n",
    "        \n",
    "        normalized[key] = value\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def detect_schema_format(data: dict) -> str:\n",
    "    \"\"\"Detect if schema is OLD or NEW format\"\"\"\n",
    "    if \"report\" in data and \"inputs\" in data:\n",
    "        return \"new\"\n",
    "    elif \"input\" in data and \"output\" in data:\n",
    "        return \"old\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "def normalize_new_format_schema(data: dict) -> dict:\n",
    "    \"\"\"Normalize NEW format schema\"\"\"\n",
    "    normalized_inputs = []\n",
    "    \n",
    "    for input_item in data.get(\"inputs\", []):\n",
    "        normalized_entities = []\n",
    "        \n",
    "        for entity in input_item.get(\"output\", []):\n",
    "            normalized_entities.append(normalize_entity_new_format(entity))\n",
    "        \n",
    "        normalized_inputs.append({\n",
    "            \"input\": input_item.get(\"input\", \"\").strip(),\n",
    "            \"output\": normalized_entities\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"report\": data.get(\"report\", \"\").strip(),\n",
    "        \"inputs\": normalized_inputs\n",
    "    }\n",
    "\n",
    "def auto_normalize_report_schemas(base_dir: str = \"data_report\"):\n",
    "    \"\"\"\n",
    "    Auto-normalize all report-level schemas\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"âŒ Base directory not found: {base_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Find all numbered directories\n",
    "    data_dirs = sorted([d for d in base_path.iterdir() if d.is_dir() and d.name.isdigit()])\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"REPORT-LEVEL SCHEMA NORMALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nBase directory: {base_dir}\")\n",
    "    print(f\"Found {len(data_dirs)} data directories: {[d.name for d in data_dirs]}\")\n",
    "    \n",
    "    total_processed = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    for data_dir in data_dirs:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Processing: {data_dir}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Find all JSON files\n",
    "        json_files = list(data_dir.glob(\"gt*.json\")) + list(data_dir.glob(\"sample*.json\"))\n",
    "        \n",
    "        if not json_files:\n",
    "            print(f\"âš  No JSON files found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found {len(json_files)} JSON files\")\n",
    "        \n",
    "        for json_file in sorted(json_files):\n",
    "            try:\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    raw_data = json.load(f)\n",
    "                \n",
    "                # Detect format\n",
    "                schema_format = detect_schema_format(raw_data)\n",
    "                \n",
    "                if schema_format == \"new\":\n",
    "                    # Normalize\n",
    "                    normalized = normalize_new_format_schema(raw_data)\n",
    "                    \n",
    "                    # Overwrite original (or save to new location)\n",
    "                    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(normalized, f, indent=2, ensure_ascii=False)\n",
    "                    \n",
    "                    print(f\"  âœ… {json_file.name} (NEW format normalized)\")\n",
    "                    total_processed += 1\n",
    "                    \n",
    "                elif schema_format == \"old\":\n",
    "                    print(f\"  â„¹ï¸  {json_file.name} (OLD format - skipped)\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"  âš ï¸  {json_file.name} (Unknown format)\")\n",
    "                    total_errors += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ {json_file.name}: {e}\")\n",
    "                total_errors += 1\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"NORMALIZATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Processed: {total_processed}\")\n",
    "    print(f\"Errors: {total_errors}\")\n",
    "    \n",
    "    return total_processed, total_errors\n",
    "\n",
    "# RUN\n",
    "print(\"ğŸ”„ Normalizing report-level schemas...\\n\")\n",
    "processed, errors = auto_normalize_report_schemas(\"data_report\")\n",
    "\n",
    "if errors == 0:\n",
    "    print(\"\\nâœ… ALL SCHEMAS NORMALIZED!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  {errors} errors occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c38e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… sentence-transformers yÃ¼klendi!\n",
      "âœ… Evaluator initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from medical_schema_evaluator import MedicalSchemaEvaluator\n",
    "import importlib\n",
    "import medical_schema_evaluator\n",
    "\n",
    "importlib.reload(medical_schema_evaluator)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"âœ… sentence-transformers yÃ¼klendi!\")\n",
    "\n",
    "# Test initialization\n",
    "evaluator = MedicalSchemaEvaluator()\n",
    "print(\"âœ… Evaluator initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ENTITY-LEVEL EVALUATION v2.0 (Semantic Matching)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‚ Data Directory: ./data_report/0/\n",
      "ğŸ“„ Ground Truth: gt0.json\n",
      "\n",
      "ğŸ¤– Selected LLM Models (1):\n",
      "   âœ… gemini_pro: models/gemini-2.5-pro\n",
      "\n",
      "ğŸ§  Selected Embedding Models (4):\n",
      "   âœ… pubmedbert: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "   âœ… s_pubmedbert: pritamdeka/S-PubMedBert-MS-MARCO\n",
      "   âœ… general_baseline: sentence-transformers/all-MiniLM-L6-v2\n",
      "   âœ… neuml_pubmedbert: NeuML/pubmedbert-base-embeddings\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING EVALUATION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "BATCH EVALUATION\n",
      "======================================================================\n",
      "Directory: ./data_report/0/\n",
      "GT: gt0.json\n",
      "Test files: 4\n",
      "\n",
      "ğŸ”§ Initializing Semantic Medical Matcher...\n",
      "âš¡ Using cached model: cambridgeltl/SapBERT-from-PubMedBERT-fulltext\n",
      "âœ… SapBERT loaded\n",
      "âš¡ Using cached model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "âœ… PubMedBERT loaded\n",
      "âš¡ Using cached model: cambridgeltl/SapBERT-from-PubMedBERT-fulltext\n",
      "âœ… SapBERT loaded\n",
      "âš¡ Using cached model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "âœ… PubMedBERT loaded\n",
      "âœ… Evaluator ready!\n",
      "\n",
      "======================================================================\n",
      "Evaluating: sample0.0.json\n",
      "======================================================================\n",
      "GT entities: 30 | Pred entities: 30\n",
      "\n",
      "ğŸ“Š ENTITY-LEVEL RESULTS:\n",
      "  Precision:  0.900\n",
      "  Recall:     0.900\n",
      "  F1-Score:   0.900\n",
      "\n",
      "ğŸ” EMBEDDING EVALUATION:\n",
      "\n",
      "ğŸ¤– LLM EVALUATION:\n",
      "\n",
      "ğŸ” DETAILED ANALYSIS:\n",
      "  False Positives: 3\n",
      "  False Negatives: 3\n",
      "     FN: Surface irregularity - Dissection @ ['thoracic aorta']\n",
      "     FN: Surface irregularity - Penetrating atheromatous ulcer @ ['thoracic aorta']\n",
      "     FN: Cholecystitis - None @ ['gallbladder']\n",
      "\n",
      "ğŸ“ˆ COMPARISON SUMMARY:\n",
      "  Structural (Entity-Level): F1=0.900\n",
      "\n",
      "  ğŸ’¾ Saved: result_sample0.0.json\n",
      "\n",
      "======================================================================\n",
      "Evaluating: sample0.1.json\n",
      "======================================================================\n",
      "GT entities: 30 | Pred entities: 30\n",
      "\n",
      "ğŸ“Š ENTITY-LEVEL RESULTS:\n",
      "  Precision:  0.300\n",
      "  Recall:     0.300\n",
      "  F1-Score:   0.300\n",
      "\n",
      "ğŸ” EMBEDDING EVALUATION:\n",
      "\n",
      "ğŸ¤– LLM EVALUATION:\n",
      "\n",
      "ğŸ” DETAILED ANALYSIS:\n",
      "  False Positives: 21\n",
      "  False Negatives: 21\n",
      "     FN: Effusion - Pericardial effusion @ ['pericardium']\n",
      "     FN: Calcification - Coronary artery calcification @ ['coronary artery']\n",
      "     FN: Aortic dilatation - None @ ['ascending aorta']\n",
      "\n",
      "ğŸ“ˆ COMPARISON SUMMARY:\n",
      "  Structural (Entity-Level): F1=0.300\n",
      "\n",
      "  ğŸ’¾ Saved: result_sample0.1.json\n",
      "\n",
      "======================================================================\n",
      "Evaluating: sample0.2.json\n",
      "======================================================================\n",
      "GT entities: 30 | Pred entities: 30\n",
      "\n",
      "ğŸ“Š ENTITY-LEVEL RESULTS:\n",
      "  Precision:  0.733\n",
      "  Recall:     0.733\n",
      "  F1-Score:   0.733\n",
      "\n",
      "ğŸ” EMBEDDING EVALUATION:\n",
      "\n",
      "ğŸ¤– LLM EVALUATION:\n",
      "\n",
      "ğŸ” DETAILED ANALYSIS:\n",
      "  False Positives: 8\n",
      "  False Negatives: 8\n",
      "     FN: Anatomical variant - Bovine aortic arch anatomy @ ['aortic arch']\n",
      "     FN: Airway obstruction - None @ ['trachea', 'central airways']\n",
      "     FN: Lesion - Osseous lesion @ ['osseous structures']\n",
      "\n",
      "ğŸ“ˆ COMPARISON SUMMARY:\n",
      "  Structural (Entity-Level): F1=0.733\n",
      "\n",
      "  ğŸ’¾ Saved: result_sample0.2.json\n",
      "\n",
      "======================================================================\n",
      "Evaluating: sample0.3.json\n",
      "======================================================================\n",
      "GT entities: 30 | Pred entities: 18\n",
      "\n",
      "ğŸ“Š ENTITY-LEVEL RESULTS:\n",
      "  Precision:  0.333\n",
      "  Recall:     0.200\n",
      "  F1-Score:   0.250\n",
      "\n",
      "ğŸ” EMBEDDING EVALUATION:\n",
      "\n",
      "ğŸ¤– LLM EVALUATION:\n",
      "\n",
      "ğŸ” DETAILED ANALYSIS:\n",
      "  False Positives: 12\n",
      "  False Negatives: 24\n",
      "     FN: Thyroid abnormality - None @ ['thyroid gland']\n",
      "     FN: Effusion - Pericardial effusion @ ['pericardium']\n",
      "     FN: Calcification - Coronary artery calcification @ ['coronary artery']\n",
      "\n",
      "ğŸ“ˆ COMPARISON SUMMARY:\n",
      "  Structural (Entity-Level): F1=0.250\n",
      "\n",
      "  ğŸ’¾ Saved: result_sample0.3.json\n",
      "\n",
      "ğŸ“„ Summary: data_report/0/entity_level_results/SUMMARY.txt\n",
      "\n",
      "======================================================================\n",
      "âœ… EVALUATION COMPLETE\n",
      "Results: data_report/0/entity_level_results\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š COMPREHENSIVE COMPARISON TABLE\n",
      "======================================================================\n",
      "Sample       Structural   PubMedBERT   S-PubMedBERT   LLM       \n",
      "----------------------------------------------------------------------\n",
      "sample0.0.json 0.900        0.000        0.000          0.000     \n",
      "sample0.1.json 0.300        0.000        0.000          0.000     \n",
      "sample0.2.json 0.733        0.000        0.000          0.000     \n",
      "sample0.3.json 0.250        0.000        0.000          0.000     \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "âœ… DONE!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: ENTITY-LEVEL EVALUATION WITH SEMANTIC MATCHING + ERROR DETECTION\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Entity-level evaluation with SapBERT semantic matching\n",
    "+ Structural Error Detection (Merged/Split entities)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT EVALUATORS\n",
    "# ============================================================================\n",
    "\n",
    "from entity_level_evaluator import EntityLevelEvaluator, SemanticMedicalMatcher\n",
    "from llm_evaluator import LLMEvaluator\n",
    "from multi_embedding_evaluator import EmbeddingEvaluator\n",
    "\n",
    "# ============================================================================\n",
    "# ğŸ†• YENÄ° EKLENEN: Structural Error Detection FonksiyonlarÄ±\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# â­ USER CONFIGURATION (AynÄ± kalÄ±yor)\n",
    "# ============================================================================\n",
    "class UserConfig:\n",
    "    \"\"\"\n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    ğŸ“ EDIT THESE SETTINGS TO CONTROL WHICH MODELS TO USE\n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"\"\"\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # ğŸ”‘ API KEYS - Add your keys here\n",
    "    # ----------------------------------------------------------------\n",
    "    API_KEYS = {\n",
    "        \"gemini\": \"AIzaSyCKbu90y_CTrfEc5fEKuWTYha2FvwfySVA\",\n",
    "        #\"gemma\": \"\",           \n",
    "        #\"glm\": \"\",             \n",
    "        #\"deepseek\": \"\",        \n",
    "    }\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # ğŸ¤– LLM MODELS TO USE\n",
    "    # ----------------------------------------------------------------\n",
    "    SELECTED_LLM_MODELS = [\n",
    "        \"gemini_pro\",          # âœ… USING THIS\n",
    "    ]\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # ğŸ§  EMBEDDING MODELS TO USE\n",
    "    # ----------------------------------------------------------------\n",
    "    SELECTED_EMBEDDING_MODELS = [\n",
    "        \"pubmedbert\",           \n",
    "        \"s_pubmedbert\",         \n",
    "        \"general_baseline\",   \n",
    "        \"neuml_pubmedbert\",   \n",
    "    ]\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # ğŸ“ DATA CONFIGURATION\n",
    "    # ----------------------------------------------------------------\n",
    "    DATA_DIR = \"./data_report/0/\"\n",
    "    GT_FILENAME = \"gt0.json\"\n",
    "    OUTPUT_DIR = None  # Auto-generated if None\n",
    "    \n",
    "    # Semantic matching threshold\n",
    "    MATCH_THRESHOLD = 0.6  # Minimum score to consider entities matched\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# END OF USER CONFIGURATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def detect_entity_structural_errors(matches: List[Dict], gt_entities: List[Dict], pred_entities: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    YapÄ±sal hatalarÄ± tespit et:\n",
    "    1. MERGED: 2+ GT entity -> 1 Pred entity (Ã¶rnekteki Cardiomegaly+Effusion durumu)\n",
    "    2. SPLIT: 1 GT entity -> 2+ Pred entity  \n",
    "    3. DEGREE_MIXUP: Uyumsuz degree kombinasyonu ([\"mild\", \"trace\"])\n",
    "    \"\"\"\n",
    "    \n",
    "    errors = {\n",
    "        'merged_entities': [],      \n",
    "        'split_entities': [],       \n",
    "        'degree_mixups': [],        \n",
    "        'contradictions': []        \n",
    "    }\n",
    "    \n",
    "    # Helper: Pred entity index'ini bul\n",
    "    def get_pred_idx(pred_ent):\n",
    "        try:\n",
    "            return pred_entities.index(pred_ent)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    # 1. MERGED ENTITY Detection\n",
    "    pred_to_gt = defaultdict(list)\n",
    "    \n",
    "    for match in matches:\n",
    "        if match['match_type'] == 'matched' and match.get('pred_entity'):\n",
    "            pred_idx = get_pred_idx(match['pred_entity'])\n",
    "            if pred_idx is not None:\n",
    "                pred_to_gt[pred_idx].append({\n",
    "                    'gt_entity': match['gt_entity'],\n",
    "                    'match_score': match.get('match_score', 0)\n",
    "                })\n",
    "    \n",
    "    # BirleÅŸtirilmiÅŸ entity'leri tespit et\n",
    "    for pred_idx, gt_matches in pred_to_gt.items():\n",
    "        if len(gt_matches) > 1:\n",
    "            pred_ent = pred_entities[pred_idx]\n",
    "            pred_finding = pred_ent.get('general_finding', 'Unknown')\n",
    "            pred_degree = pred_ent.get('degree', [])\n",
    "            \n",
    "            # GT bilgilerini topla\n",
    "            gt_findings = []\n",
    "            for gm in gt_matches:\n",
    "                gt_ent = gm['gt_entity']\n",
    "                gt_findings.append({\n",
    "                    'finding': gt_ent.get('general_finding'),\n",
    "                    'specific': gt_ent.get('specific_finding'),\n",
    "                    'degree': gt_ent.get('degree', []),\n",
    "                    'location': gt_ent.get('location', [])\n",
    "                })\n",
    "            \n",
    "            error_info = {\n",
    "                'type': 'MERGED_ENTITY',\n",
    "                'severity': 'HIGH',\n",
    "                'description': f\"{len(gt_matches)} farklÄ± GT entity tek entity'de birleÅŸtirilmiÅŸ\",\n",
    "                'gt_entities': gt_findings,\n",
    "                'pred_entity': {\n",
    "                    'finding': pred_finding,\n",
    "                    'degree': pred_degree,\n",
    "                    'location': pred_ent.get('location', [])\n",
    "                },\n",
    "                'impact': f\"{len(gt_matches)-1} entity kayÄ±p (FN)\"\n",
    "            }\n",
    "            errors['merged_entities'].append(error_info)\n",
    "            \n",
    "            # Degree mixup kontrolÃ¼\n",
    "            if len(pred_degree) > 1:\n",
    "                # Uyumsuz degree kontrolÃ¼\n",
    "                degree_set = set(str(d).lower() for d in pred_degree)\n",
    "                exclusive = {'mild', 'moderate', 'severe', 'trace', 'small', 'large'}\n",
    "                if len(degree_set & exclusive) > 1:\n",
    "                    errors['degree_mixups'].append({\n",
    "                        'finding': pred_finding,\n",
    "                        'degrees': pred_degree,\n",
    "                        'source_entities': [g['finding'] for g in gt_findings],\n",
    "                        'reason': \"FarklÄ± entity'lerden gelen degree'ler birleÅŸtirilmiÅŸ\"\n",
    "                    })\n",
    "    \n",
    "    # 2. SPLIT ENTITY Detection\n",
    "    gt_to_pred = defaultdict(list)\n",
    "    for i, match in enumerate(matches):\n",
    "        if match['match_type'] == 'matched' and match.get('gt_entity'):\n",
    "            try:\n",
    "                gt_idx = gt_entities.index(match['gt_entity'])\n",
    "                gt_to_pred[gt_idx].append(match['pred_entity'])\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    for gt_idx, pred_ents in gt_to_pred.items():\n",
    "        if len(pred_ents) > 1:\n",
    "            gt_ent = gt_entities[gt_idx]\n",
    "            errors['split_entities'].append({\n",
    "                'type': 'SPLIT_ENTITY',\n",
    "                'severity': 'MEDIUM',\n",
    "                'gt_entity': {\n",
    "                    'finding': gt_ent.get('general_finding'),\n",
    "                    'degree': gt_ent.get('degree')\n",
    "                },\n",
    "                'split_into': [p.get('general_finding') for p in pred_ents],\n",
    "                'count': len(pred_ents)\n",
    "            })\n",
    "    \n",
    "    # 3. CONTRADICTION Detection\n",
    "    for match in matches:\n",
    "        if match['match_type'] == 'matched':\n",
    "            gt_ent = match['gt_entity']\n",
    "            pred_ent = match['pred_entity']\n",
    "            \n",
    "            gt_pres = str(gt_ent.get('finding_presence', '')).lower()\n",
    "            pred_pres = str(pred_ent.get('finding_presence', '')).lower()\n",
    "            \n",
    "            contradictions = [('present', 'absent'), ('absent', 'present'), ('normal', 'abnormal')]\n",
    "            if (gt_pres, pred_pres) in contradictions:\n",
    "                errors['contradictions'].append({\n",
    "                    'finding': gt_ent.get('general_finding'),\n",
    "                    'gt_presence': gt_pres,\n",
    "                    'pred_presence': pred_pres\n",
    "                })\n",
    "    \n",
    "    return errors\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL CONFIGURATIONS\n",
    "# ============================================================================\n",
    "\n",
    "EMBEDDING_MODELS = {\n",
    "    \"pubmedbert\": {\n",
    "        \"name\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        \"description\": \"PubMed trained\"\n",
    "    },\n",
    "    \"s_pubmedbert\": {\n",
    "        \"name\": \"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        \"description\": \"Sentence-level\"\n",
    "    },\n",
    "    \"general_baseline\": {\n",
    "        \"name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"description\": \"General baseline\"\n",
    "    },\n",
    "    \"neuml_pubmedbert\": {\n",
    "        \"name\": \"NeuML/pubmedbert-base-embeddings\",\n",
    "        \"description\": \"NeuML embeddings\"\n",
    "    }\n",
    "}\n",
    "\n",
    "LLM_MODELS = {\n",
    "    #\"gemini_flash\": {\"type\": \"gemini\", \"name\": \"models/gemini-2.5-flash\"},\n",
    "    \"gemini_pro\": {\"type\": \"gemini\", \"name\": \"models/gemini-2.5-pro\"},\n",
    "    #\"gemma\": {\"type\": \"gemma\", \"name\": \"gemma-3-27b-it\"},\n",
    "    #\"glm\": {\"type\": \"glm\", \"name\": \"glm-4-flash\"},\n",
    "    #\"deepseek\": {\"type\": \"deepseek\", \"name\": \"deepseek-chat\"}\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION & DISPLAY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTITY-LEVEL EVALUATION v2.0 (Semantic Matching)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Validate selections\n",
    "valid_llm_keys = set(LLM_MODELS.keys())\n",
    "invalid_llm = [k for k in UserConfig.SELECTED_LLM_MODELS if k not in valid_llm_keys]\n",
    "if invalid_llm:\n",
    "    print(f\"âš ï¸  WARNING: Invalid LLM models: {invalid_llm}\")\n",
    "    UserConfig.SELECTED_LLM_MODELS = [k for k in UserConfig.SELECTED_LLM_MODELS if k in valid_llm_keys]\n",
    "\n",
    "valid_emb_keys = set(EMBEDDING_MODELS.keys())\n",
    "invalid_emb = [k for k in UserConfig.SELECTED_EMBEDDING_MODELS if k not in valid_emb_keys]\n",
    "if invalid_emb:\n",
    "    print(f\"âš ï¸  WARNING: Invalid embedding models: {invalid_emb}\")\n",
    "    UserConfig.SELECTED_EMBEDDING_MODELS = [k for k in UserConfig.SELECTED_EMBEDDING_MODELS if k in valid_emb_keys]\n",
    "\n",
    "print(f\"\\nğŸ“‚ Data Directory: {UserConfig.DATA_DIR}\")\n",
    "print(f\"ğŸ“„ Ground Truth: {UserConfig.GT_FILENAME}\")\n",
    "print(f\"\\nğŸ¤– Selected LLM Models ({len(UserConfig.SELECTED_LLM_MODELS)}):\")\n",
    "for llm in UserConfig.SELECTED_LLM_MODELS:\n",
    "    llm_info = LLM_MODELS[llm]\n",
    "    has_key = \"âœ…\" if UserConfig.API_KEYS.get(llm_info['type']) else \"âŒ\"\n",
    "    print(f\"   {has_key} {llm}: {llm_info['name']}\")\n",
    "\n",
    "print(f\"\\nğŸ§  Selected Embedding Models ({len(UserConfig.SELECTED_EMBEDDING_MODELS)}):\")\n",
    "for emb in UserConfig.SELECTED_EMBEDDING_MODELS:\n",
    "    emb_info = EMBEDDING_MODELS[emb]\n",
    "    print(f\"   âœ… {emb}: {emb_info['name']}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION CLASS\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class EntityLevelReportEvaluator:\n",
    "    \n",
    "    def __init__(self, api_keys: dict, match_threshold: float = 0.6):\n",
    "        self.api_keys = api_keys\n",
    "        self.match_threshold = match_threshold\n",
    "        \n",
    "        print(\"\\nğŸ”§ Initializing Semantic Medical Matcher...\")\n",
    "        self.semantic_matcher = SemanticMedicalMatcher(use_embeddings=True)\n",
    "        \n",
    "        self.entity_evaluator = EntityLevelEvaluator(\n",
    "            use_semantic_matching=True,\n",
    "            use_llm_for_borderline=False,\n",
    "            llm_evaluator=None\n",
    "        )\n",
    "        print(\"âœ… Evaluator ready!\")\n",
    "    \n",
    "    def evaluate_single(self, \n",
    "                       gt_path: str, \n",
    "                       pred_path: str,\n",
    "                       embedding_models: List[str] = None,\n",
    "                       llm_models: List[str] = None) -> Dict:\n",
    "        \n",
    "        # Load schemas\n",
    "        with open(gt_path, 'r') as f:\n",
    "            gt_schema = json.load(f)\n",
    "        with open(pred_path, 'r') as f:\n",
    "            pred_schema = json.load(f)\n",
    "        \n",
    "        result = {\n",
    "            'gt_file': Path(gt_path).name,\n",
    "            'pred_file': Path(pred_path).name,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Validate report\n",
    "        gt_report = gt_schema.get('report', '').strip()\n",
    "        pred_report = pred_schema.get('report', '').strip()\n",
    "        \n",
    "        if gt_report != pred_report:\n",
    "            return {**result, 'status': 'error', 'error': 'Reports do not match'}\n",
    "        \n",
    "        result['status'] = 'success'\n",
    "        \n",
    "        # Flatten entities\n",
    "        gt_entities = self.entity_evaluator.flatten_entities(gt_schema)\n",
    "        pred_entities = self.entity_evaluator.flatten_entities(pred_schema)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Evaluating: {Path(pred_path).name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"GT entities: {len(gt_entities)} | Pred entities: {len(pred_entities)}\")\n",
    "        \n",
    "        # Match entities\n",
    "        matches = self.entity_evaluator.match_entities(\n",
    "            gt_entities, \n",
    "            pred_entities,\n",
    "            report_text=gt_report\n",
    "        )\n",
    "        \n",
    "        # ğŸ†• DEÄÄ°ÅÄ°KLÄ°K 1: Standart metrikler + YapÄ±sal hata analizi birlikte\n",
    "        base_metrics = self.entity_evaluator.compute_metrics(matches)\n",
    "        structural_errors = detect_entity_structural_errors(matches, gt_entities, pred_entities)\n",
    "        \n",
    "        # ğŸ†• DEÄÄ°ÅÄ°KLÄ°K 2: Hata Ã¶zetini oluÅŸtur\n",
    "        error_summary = {\n",
    "            'merged_count': len(structural_errors['merged_entities']),\n",
    "            'split_count': len(structural_errors['split_entities']),\n",
    "            'degree_mixups': len(structural_errors['degree_mixups']),\n",
    "            'contradictions': len(structural_errors['contradictions'])\n",
    "        }\n",
    "        \n",
    "        result['entity_metrics'] = {\n",
    "            'total_gt': len(gt_entities),\n",
    "            'total_pred': len(pred_entities),\n",
    "            'true_positives': base_metrics['true_positives'],\n",
    "            'false_positives': base_metrics['false_positives'],\n",
    "            'false_negatives': base_metrics['false_negatives'],\n",
    "            'precision': base_metrics['precision'],\n",
    "            'recall': base_metrics['recall'],\n",
    "            'f1_score': base_metrics['f1_score'],\n",
    "            'avg_match_quality': base_metrics['avg_match_quality'],\n",
    "            'contradictions': base_metrics['contradiction_count'],\n",
    "            'structural_errors': error_summary,  # ğŸ†• Yeni eklendi\n",
    "            'has_structural_errors': any(v > 0 for v in error_summary.values())\n",
    "        }\n",
    "        \n",
    "        # ğŸ†• DEÄÄ°ÅÄ°KLÄ°K 3: DetaylÄ± hata raporlamasÄ±\n",
    "        print(f\"\\nğŸ“Š ENTITY-LEVEL RESULTS:\")\n",
    "        print(f\"  Precision:  {base_metrics['precision']:.3f}\")\n",
    "        print(f\"  Recall:     {base_metrics['recall']:.3f}\")\n",
    "        print(f\"  F1-Score:   {base_metrics['f1_score']:.3f}\")\n",
    "        \n",
    "        # ğŸ†• YENÄ° EKLENEN: Structural Error Raporlama\n",
    "        if error_summary['merged_count'] > 0:\n",
    "            print(f\"\\nğŸš¨ MERGED ENTITIES DETECTED: {error_summary['merged_count']}\")\n",
    "            for err in structural_errors['merged_entities'][:2]:  # Ä°lk 2'yi gÃ¶ster\n",
    "                gt_names = [e['finding'] for e in err['gt_entities']]\n",
    "                print(f\"   âŒ {err['description']}\")\n",
    "                print(f\"      GT: {gt_names}\")\n",
    "                print(f\"      Pred: {err['pred_entity']['finding']} {err['pred_entity']['degree']}\")\n",
    "                print(f\"      Impact: {err['impact']}\")\n",
    "        \n",
    "        if error_summary['degree_mixups'] > 0:\n",
    "            print(f\"\\nâš ï¸  DEGREE MIXUPS: {error_summary['degree_mixups']}\")\n",
    "            for mix in structural_errors['degree_mixups'][:2]:\n",
    "                print(f\"   ğŸ”´ {mix['finding']}: {mix['degrees']}\")\n",
    "                print(f\"      Reason: {mix['reason']}\")\n",
    "        \n",
    "        if error_summary['split_count'] > 0:\n",
    "            print(f\"\\nâš ï¸  SPLIT ENTITIES: {error_summary['split_count']}\")\n",
    "        \n",
    "        # Embedding evaluation (AynÄ± kalÄ±yor)\n",
    "        if embedding_models:\n",
    "            print(f\"\\nğŸ” EMBEDDING EVALUATION:\")\n",
    "            result['embedding_scores'] = {}\n",
    "            # ğŸ†• EMBEDDING SONUÃ‡LARINI YAZDIR\n",
    "            for emb_key, emb_data in result.get('embedding_scores', {}).items():\n",
    "                print(f\"  ğŸ“Š {emb_key}: {emb_data['mean']:.3f} (Â±{emb_data['std']:.3f})\")\n",
    "        \n",
    "        # LLM evaluation (AynÄ± kalÄ±yor)\n",
    "        if llm_models:\n",
    "            print(f\"\\nğŸ¤– LLM EVALUATION:\")\n",
    "            # ... (llm kodlarÄ± aynÄ±) ...\n",
    "        \n",
    "        # ğŸ†• DEÄÄ°ÅÄ°KLÄ°K 4: Sample mismatches + structural errors birlikte gÃ¶ster\n",
    "        print(f\"\\nğŸ” DETAILED ANALYSIS:\")\n",
    "        \n",
    "        # False Positive/Negative Ã¶zetini gÃ¶ster (eski kod aynÄ±)\n",
    "        fp_count = sum(1 for m in matches if m['match_type'] == 'false_positive')\n",
    "        fn_count = sum(1 for m in matches if m['match_type'] == 'false_negative')\n",
    "        print(f\"  False Positives: {fp_count}\")\n",
    "        print(f\"  False Negatives: {fn_count}\")\n",
    "        \n",
    "        # Sample FN'leri gÃ¶ster\n",
    "        fn_shown = 0\n",
    "        for match in matches:\n",
    "            if match['match_type'] == 'false_negative' and fn_shown < 3:\n",
    "                ent = match['gt_entity']\n",
    "                print(f\"     FN: {ent.get('general_finding')} - {ent.get('specific_finding')} @ {ent.get('location')}\")\n",
    "                fn_shown += 1\n",
    "        \n",
    "        # ğŸ†• YENÄ°: EÄŸer FN'ler merged entity'den kaynaklanÄ±yorsa belirt\n",
    "        if error_summary['merged_count'] > 0 and fn_count > 0:\n",
    "            print(f\"     âš ï¸  Not: BazÄ± FN'ler merged entity'lerden kaynaklanÄ±yor olabilir\")\n",
    "        \n",
    "        result['detailed_matches'] = matches\n",
    "        result['structural_error_details'] = structural_errors  # ğŸ†• Tam detaylarÄ± da sakla\n",
    "        # evaluate_single metodu sonuna, return result'dan Ã¶nce ekleyin:\n",
    "\n",
    "        print(f\"\\nğŸ“ˆ COMPARISON SUMMARY:\")\n",
    "        print(f\"  Structural (Entity-Level): F1={base_metrics['f1_score']:.3f}\")\n",
    "\n",
    "        if 'embedding_scores' in result:\n",
    "            for emb_name, emb_data in result['embedding_scores'].items():\n",
    "                print(f\"  Semantic ({emb_name}): {emb_data['mean']:.3f}\")\n",
    "\n",
    "        if 'llm_scores' in result:\n",
    "            for llm_name, llm_data in result['llm_scores'].items():\n",
    "                print(f\"  LLM ({llm_name}): {llm_data['mean']:.3f}\")\n",
    "        return result\n",
    "\n",
    "def evaluate_directory(\n",
    "    data_dir: str = None,\n",
    "    gt_filename: str = None,\n",
    "    output_dir: str = None,\n",
    "    embedding_models: List[str] = None,\n",
    "    llm_models: List[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate all samples in directory\n",
    "    \"\"\"\n",
    "    # Defaults\n",
    "    data_dir = data_dir or UserConfig.DATA_DIR\n",
    "    gt_filename = gt_filename or UserConfig.GT_FILENAME\n",
    "    output_dir = output_dir or (Path(data_dir) / 'entity_level_results')\n",
    "    embedding_models = embedding_models or UserConfig.SELECTED_EMBEDDING_MODELS\n",
    "    llm_models = llm_models or UserConfig.SELECTED_LLM_MODELS\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    gt_path = data_path / gt_filename\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not gt_path.exists():\n",
    "        print(f\"âŒ GT file not found: {gt_path}\")\n",
    "        return\n",
    "    \n",
    "    # Find test files\n",
    "    test_files = sorted(data_path.glob(\"sample*.json\"))\n",
    "    \n",
    "    if not test_files:\n",
    "        print(f\"âš ï¸ No test files found in {data_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BATCH EVALUATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Directory: {data_dir}\")\n",
    "    print(f\"GT: {gt_filename}\")\n",
    "    print(f\"Test files: {len(test_files)}\")\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = EntityLevelReportEvaluator(\n",
    "        api_keys=UserConfig.API_KEYS,\n",
    "        match_threshold=UserConfig.MATCH_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # Evaluate each file\n",
    "    all_results = []\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        result = evaluator.evaluate_single(\n",
    "            str(gt_path),\n",
    "            str(test_file),\n",
    "            embedding_models=embedding_models,\n",
    "            llm_models=llm_models\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save individual result\n",
    "        result_file = output_path / f\"result_{test_file.stem}.json\"\n",
    "        with open(result_file, 'w') as f:\n",
    "            # Don't save full matches to keep file size reasonable\n",
    "            result_summary = {k: v for k, v in result.items() if k != 'detailed_matches'}\n",
    "            json.dump(result_summary, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n  ğŸ’¾ Saved: {result_file.name}\")\n",
    "    \n",
    "    # Generate summary\n",
    "    _generate_summary(all_results, output_path)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"âœ… EVALUATION COMPLETE\")\n",
    "    print(f\"Results: {output_path}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def _generate_summary(results: List[Dict], output_dir: Path):\n",
    "    \"\"\"Generate summary report\"\"\"\n",
    "    summary_path = output_dir / \"SUMMARY.txt\"\n",
    "    \n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        f.write(\"ENTITY-LEVEL EVALUATION SUMMARY\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
    "        f.write(f\"Total samples: {len(results)}\\n\\n\")\n",
    "        \n",
    "        # Per-sample results\n",
    "        f.write(\"PER-SAMPLE RESULTS:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        \n",
    "        for result in results:\n",
    "            if result.get('status') == 'error':\n",
    "                f.write(f\"\\n{result['pred_file']}: âŒ ERROR - {result['error']}\\n\")\n",
    "                continue\n",
    "            \n",
    "            metrics = result['entity_metrics']\n",
    "            f.write(f\"\\n{result['pred_file']}:\\n\")\n",
    "            f.write(f\"  Entities: GT={metrics['total_gt']}, Pred={metrics['total_pred']}\\n\")\n",
    "            f.write(f\"  Precision: {metrics['precision']:.3f}\\n\")\n",
    "            f.write(f\"  Recall:    {metrics['recall']:.3f}\\n\")\n",
    "            f.write(f\"  F1-Score:  {metrics['f1_score']:.3f}\\n\")\n",
    "            f.write(f\"  TP: {metrics['true_positives']}, FP: {metrics['false_positives']}, FN: {metrics['false_negatives']}\\n\")\n",
    "            \n",
    "            if metrics['contradictions'] > 0:\n",
    "                f.write(f\"  âš ï¸ Contradictions: {metrics['contradictions']}\\n\")\n",
    "            \n",
    "            # Embedding scores\n",
    "            if 'embedding_scores' in result:\n",
    "                f.write(f\"  Embeddings:\\n\")\n",
    "                for emb_name, emb_data in result['embedding_scores'].items():\n",
    "                    f.write(f\"    {emb_name}: {emb_data['mean']:.3f}\\n\")\n",
    "            \n",
    "            # LLM scores\n",
    "            if 'llm_scores' in result:\n",
    "                f.write(f\"  LLM:\\n\")\n",
    "                for llm_name, llm_data in result['llm_scores'].items():\n",
    "                    f.write(f\"    {llm_name}: {llm_data['mean']:.3f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        f.write(\"STRUCTURAL ERROR ANALYSIS\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        \n",
    "        total_merged = sum(r['entity_metrics'].get('structural_errors', {}).get('merged_count', 0) \n",
    "                          for r in results if r.get('status') == 'success')\n",
    "        total_splits = sum(r['entity_metrics'].get('structural_errors', {}).get('split_count', 0) \n",
    "                          for r in results if r.get('status') == 'success')\n",
    "        total_mixups = sum(r['entity_metrics'].get('structural_errors', {}).get('degree_mixups', 0) \n",
    "                          for r in results if r.get('status') == 'success')\n",
    "        \n",
    "        f.write(f\"Total Merged Entities: {total_merged}\\n\")\n",
    "        f.write(f\"Total Split Entities: {total_splits}\\n\")\n",
    "        f.write(f\"Total Degree Mixups: {total_mixups}\\n\")\n",
    "        \n",
    "        if total_merged > 0:\n",
    "            f.write(f\"\\nâš ï¸  Merged entity'ler recall dÃ¼ÅŸÃ¼klÃ¼ÄŸÃ¼ne neden oluyor!\\n\")\n",
    "            f.write(f\"    (Ã–rn: Cardiomegaly[mild] + Effusion[trace] â†’ Cardiomegaly[mild,trace])\\n\")\n",
    "            \n",
    "        # Aggregate statistics\n",
    "        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        f.write(\"AGGREGATE STATISTICS\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        \n",
    "        successful = [r for r in results if r.get('status') == 'success']\n",
    "        if successful:\n",
    "            avg_precision = np.mean([r['entity_metrics']['precision'] for r in successful])\n",
    "            avg_recall = np.mean([r['entity_metrics']['recall'] for r in successful])\n",
    "            avg_f1 = np.mean([r['entity_metrics']['f1_score'] for r in successful])\n",
    "            \n",
    "            f.write(f\"\\nAverage Precision: {avg_precision:.3f}\\n\")\n",
    "            f.write(f\"Average Recall:    {avg_recall:.3f}\\n\")\n",
    "            f.write(f\"Average F1-Score:  {avg_f1:.3f}\\n\")\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Summary: {summary_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ STARTING EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ESKÄ°:\n",
    "# results = evaluate_directory()\n",
    "# print(\"\\nâœ… DONE!\")\n",
    "\n",
    "# YENÄ°:\n",
    "results = evaluate_directory()\n",
    "\n",
    "# ğŸ†• KARÅILAÅTIRMALI TABLO\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š COMPREHENSIVE COMPARISON TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Sample':<12} {'Structural':<12} {'PubMedBERT':<12} {'S-PubMedBERT':<14} {'LLM':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for r in results:\n",
    "    if r.get('status') != 'success':\n",
    "        continue\n",
    "    \n",
    "    sample = r['pred_file']\n",
    "    struct_f1 = r['entity_metrics']['f1_score']\n",
    "    \n",
    "    # Embedding ortalamalarÄ±\n",
    "    emb_pub = r.get('embedding_scores', {}).get('pubmedbert', {}).get('mean', 0)\n",
    "    emb_spub = r.get('embedding_scores', {}).get('s_pubmedbert', {}).get('mean', 0)\n",
    "    \n",
    "    # LLM\n",
    "    llm = r.get('llm_scores', {}).get('gemini_pro', {}).get('mean', 0)\n",
    "    \n",
    "    print(f\"{sample:<12} {struct_f1:<12.3f} {emb_pub:<12.3f} {emb_spub:<14.3f} {llm:<10.3f}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(\"\\nâœ… DONE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
